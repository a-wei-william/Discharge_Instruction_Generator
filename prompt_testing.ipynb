{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70ca9268-038b-48ce-9e07-528b5ecf8782",
   "metadata": {},
   "source": [
    "# Context Relevance Prompt Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0533c46a-a040-4f78-a962-a9109fef5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _global\n",
    "from typing import List\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate, evaluate_existing\n",
    "from langsmith.schemas import Example, Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd64fe4-bb44-4b0a-b18a-222d50d46626",
   "metadata": {},
   "source": [
    "## Experimental LLM Evaluator being tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e707a71-dbb4-4389-82f6-cb1853f28e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\" Pydantic object used to format LLM output\n",
    "    * 0: irrelevant diagnosis \\n\n",
    "    * 1: correct diagnosis, but does not contain information to anser the user question \\n\n",
    "    * 2: correct diagnosis and contains information to answer the user question). \\n    \n",
    "    \"\"\"\n",
    "    score: int = Field(description=\"Documents grade based on correct diagnosis and relevant information\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "#llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67c51b93-154d-45da-8ebd-9f3391ceaca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# few shots prompt\n",
    "client = Client()\n",
    "ds = client.list_examples(dataset_name=\"prompt_test_GAS\")\n",
    "\n",
    "examples = []\n",
    "required_eg_score = [0,1,2]*2\n",
    "for example in ds:\n",
    "    if int(example.outputs[\"score\"]) in required_eg_score:\n",
    "        examples.append({\n",
    "            \"context\":example.inputs[\"context\"],\n",
    "            \"query\":example.inputs[\"query\"],\n",
    "            \"score\":example.outputs[\"score\"],\n",
    "        })\n",
    "        required_eg_score.remove(int(example.outputs[\"score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8dbbbfe-b2ce-4d87-ab14-630a0b2d4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a grader assessing the relevance of a retrieved document content to a query. \\n\n",
    "The query is a question about a medical diagnosis. \\n\n",
    "The document is a Python dictionary. The content of the document is under the \"page_content\" key in the dictionary. \\n\n",
    "Give a score for the document using the scoring system below. \\n\n",
    "\n",
    "# Scoring\n",
    "- **0**: The content does not contain any information about the queried diagnosis \\n\n",
    "- **1**: The content contains information about the queried diagnosis, but the information does not answer the query \\n\n",
    "- **2**: The content contains information about the queried diagnosis and the information answers the query). \\n\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "# RETRIEVED DOCUMENT\n",
    "{context} \n",
    "\n",
    "# QUERY\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "prompt_gradedoc = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", human),\n",
    "        (\"ai\", \"{score}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=prompt_gradedoc,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = final_prompt | structured_llm_grader\n",
    "\n",
    "def grade_docs(run, example) -> dict:\n",
    "    grade = retrieval_grader.invoke({\"query\": example.inputs[\"query\"], \"context\": example.inputs[\"context\"]})\n",
    "    return {\"key\": \"grade\", \"score\": int(grade.score), \"comment\": \"grade for doc\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96be70f-1ba0-43ac-8db7-eb6a2976d206",
   "metadata": {},
   "source": [
    "## Metrics for evaluating the LLM evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76ba5b95-0e9d-42a6-94d8-dc7857082a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare generated eval scores with ground truth eval scores\n",
    "def avg_diff(runs: list[Run], examples: list[Example]) -> dict:\n",
    "    score = 0\n",
    "    for i, (run, example) in enumerate(zip(runs, examples)):\n",
    "        score += run.feedback_stats[\"grade\"][\"avg\"] - int(example.outputs[\"score\"])\n",
    "\n",
    "    score = score / len(runs)\n",
    "    \n",
    "    return {\"key\": \"avg diff from true score\", \"score\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f0848e6-1c56-4f25-aaab-3cddd5c90119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of correct scoring\n",
    "def correctness(runs: list[Run], examples: list[Example]) -> dict:\n",
    "    score = 0\n",
    "    for i, (run, example) in enumerate(zip(runs, examples)):\n",
    "        if run.feedback_stats[\"grade\"][\"avg\"] == int(example.outputs[\"score\"]):\n",
    "            score += 1\n",
    "\n",
    "    score = score / len(runs)\n",
    "    \n",
    "    return {\"key\": \"perc of correct scoring\", \"score\": score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45825fb2-8b5e-40e9-a6ef-611a2afc3b86",
   "metadata": {},
   "source": [
    "## Full pipeline for evaluating llm evaluator for context relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "342a6c4e-2a40-422a-a455-c3f8afc76af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Prompt_testing-1369472f' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/4c17fe49-6797-40aa-a5fc-36f4809034f5/compare?selectedSessions=bc44329e-1dfc-441d-8ff0-d149bf2b94c5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed3058a1947e4924a37d92ac777589f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Prompt_testing-1369472f' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/4c17fe49-6797-40aa-a5fc-36f4809034f5/compare?selectedSessions=bc44329e-1dfc-441d-8ff0-d149bf2b94c5\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fde043c4e21410f9dc658d5c2663b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults Prompt_testing-1369472f>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate eval scores\n",
    "exp = evaluate(\n",
    "    lambda x:x,\n",
    "    data=\"prompt_test_GAS\",\n",
    "    evaluators=[grade_docs],\n",
    "    experiment_prefix=\"Prompt_testing\",\n",
    ")\n",
    "\n",
    "# generate eval score for the evaluator\n",
    "evaluate_existing(exp.experiment_name, summary_evaluators=[avg_diff, correctness])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0fcfa-69bb-4708-b91b-d946e05e2fdb",
   "metadata": {},
   "source": [
    "# Faithfulness Prompt Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d787d3d-9985-4076-8f85-bc9392d562f1",
   "metadata": {},
   "source": [
    "## Experimental LLM Evaluator being tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "179aa6bb-8c2b-494f-a204-cf26d741d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Faithfulness(BaseModel):\n",
    "    \"\"\"List facts in handout not based on ground truth\"\"\"\n",
    "    list_of_false: List[str] = Field(description=\"List of facts in handout not based on ground truth\")\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(Faithfulness)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"\n",
    "You are an expert assessor tasked with evaluating whether the generated text (provided by the user between the XML tags GENERATED TEXT) is factually based on the provided context (provided by the user between the XML tags CONTEXT).  \n",
    "\n",
    "Follow these steps:\n",
    "    Step 1: Read the provided context carefully. Understand the information presented in the context.\n",
    "    Step 2: Analyze each sentence in the GENERATED TEXT. Compare it with the provided CONTEXT to determine its factual basis. Sentences in the GENERATED TEXT that are similar (but not verbatim) to the sentences in provided CONTEXT, but are still factually aligned are considered factually based on the CONTEXT.\n",
    "    Step 3: Identify sentences in the GENERATED TEXT that are not factually based on the context (not factually supported by the provided CONTEXT or directly contradicts the provided CONTEXT.) \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "<GENERATED TEXT>\n",
    "{handout} \n",
    "</GENERATED TEXT>\n",
    "\n",
    "<CONTEXT>\n",
    "{contexts}\n",
    "</CONTEXT>\n",
    "\"\"\"\n",
    "\n",
    "prompt_faithfulness = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "grader_faithfulness = prompt_faithfulness | structured_llm_grader\n",
    "\n",
    "def grade_faithfulness(run, example):\n",
    "    try: # try to get outputs from run, otherwise it is from dataset\n",
    "        handout = run.outputs[\"handout\"]\n",
    "        contexts = run.outputs[\"contexts\"]\n",
    "    except KeyError:\n",
    "        handout = example.outputs[\"handout\"]\n",
    "        contexts = example.inputs[\"contexts\"]\n",
    "        \n",
    "    result = grader_faithfulness.invoke({\"handout\": handout, \"contexts\": contexts}).list_of_false\n",
    "    count = len(result)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"count\", \"score\": count, \"comment\": \"num infactual sentences\",\n",
    "        \"sentences\": result\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17a370-1f08-4f40-9203-17cd491e927e",
   "metadata": {},
   "source": [
    "## Metrics for evaluating the LLM evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84ebf856-aa66-48b6-a281-c2885036988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average difference of generated count vs ground truth count\n",
    "def avg_diff_count(runs: list[Run], examples: list[Example]) -> dict:\n",
    "    score = 0\n",
    "    for i, (run, example) in enumerate(zip(runs, examples)):\n",
    "        score += run.feedback_stats[\"count\"][\"avg\"] - int(example.outputs[\"count\"])\n",
    "\n",
    "    score = score / len(runs)\n",
    "    \n",
    "    return {\"key\": \"avg diff from true count\", \"score\": score}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedf16b-8e0b-4bfc-9624-36cfa64e123b",
   "metadata": {},
   "source": [
    "## Full pipeline for evaluating llm evaluator for context relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec70074b-71d5-4ded-9232-10032624bfcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Prompt_testing-dd3cd07e' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/8c0209e9-1fa7-4eae-9ee6-657491c07e75/compare?selectedSessions=9483b5b6-9039-472d-b1bd-85c1bd3bc88d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55fa685dc6734044b53500663dc6ca0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Prompt_testing-dd3cd07e' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/8c0209e9-1fa7-4eae-9ee6-657491c07e75/compare?selectedSessions=9483b5b6-9039-472d-b1bd-85c1bd3bc88d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f495d362bdd4d418ebe1e5b6223e5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running summary evaluator <function avg_diff_count at 0x29b5e24d0>: 'count'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults Prompt_testing-dd3cd07e>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate eval scores\n",
    "exp = evaluate(\n",
    "    lambda x:x,\n",
    "    data=\"Prompt_test_faithfulness\",\n",
    "    evaluators=[grade_faithfulness],\n",
    "    experiment_prefix=\"Prompt_testing\",\n",
    ")\n",
    "\n",
    "# generate eval score for the evaluator\n",
    "evaluate_existing(exp.experiment_name, summary_evaluators=[avg_diff_count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ad820-5e84-408b-9fb6-82c3aaa07a39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
