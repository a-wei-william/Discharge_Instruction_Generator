{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19c6c73-db89-4bb3-a655-443cbab8056b",
   "metadata": {},
   "source": [
    "# Last left off\n",
    "- how to give structured input to llm to improve accuracy\n",
    "- few prompt strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca9268-038b-48ce-9e07-528b5ecf8782",
   "metadata": {},
   "source": [
    "# Context Relevance Prompt Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0533c46a-a040-4f78-a962-a9109fef5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _global\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.evaluation import evaluate, evaluate_existing\n",
    "from langsmith.schemas import Example, Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd64fe4-bb44-4b0a-b18a-222d50d46626",
   "metadata": {},
   "source": [
    "## Experimental LLM Evaluator being tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e707a71-dbb4-4389-82f6-cb1853f28e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\" Pydantic object used to format LLM output\n",
    "    * 0: irrelevant diagnosis \\n\n",
    "    * 1: correct diagnosis, but does not contain information to anser the user question \\n\n",
    "    * 2: correct diagnosis and contains information to answer the user question). \\n    \n",
    "    \"\"\"\n",
    "    score: int = Field(description=\"Documents grade based on correct diagnosis and relevant information\")\n",
    "\n",
    "\n",
    "#llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8dbbbfe-b2ce-4d87-ab14-630a0b2d4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a grader assessing the relevance of a retrieved document content to a query. \\n\n",
    "The query is a question about a medical diagnosis. \\n\n",
    "The document is a Python dictionary. The content of the document is under the \"page_content\" key in the dictionary. \\n\n",
    "Give a score for the document using the scoring system below. \\n\n",
    "\n",
    "# Scoring\n",
    "- **0**: The content does not contain any information about the queried diagnosis \\n\n",
    "- **1**: The content contains information about the queried diagnosis, but the information does not answer the query \\n\n",
    "- **2**: The content contains information about the queried diagnosis and the information answers the query). \\n\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "# RETRIEVED DOCUMENT\n",
    "{document} \n",
    "\n",
    "# QUERY\n",
    "{query}\n",
    "\"\"\"\n",
    "prompt_gradedoc = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt_gradedoc | structured_llm_grader\n",
    "\n",
    "def grade_docs(run, example) -> dict:\n",
    "    \"\"\"Grades all queries and corresponding documents in the db. used in eval type 1\"\"\"\n",
    "    grade = retrieval_grader.invoke({\"query\": example.inputs[\"query\"], \"document\": example.inputs[\"context\"]})\n",
    "    return {\"key\": \"grade\", \"score\": int(grade.score), \"comment\": \"grade for doc\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96be70f-1ba0-43ac-8db7-eb6a2976d206",
   "metadata": {},
   "source": [
    "## Metrics for evaluating the LLM evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ba5b95-0e9d-42a6-94d8-dc7857082a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare generated eval scores with ground truth eval scores\n",
    "def avg_diff(runs: list[Run], examples: list[Example]) -> dict:\n",
    "    score = 0\n",
    "    for i, (run, example) in enumerate(zip(runs, examples)):\n",
    "        score += run.feedback_stats[\"grade\"][\"avg\"] - int(example.outputs[\"score\"])\n",
    "\n",
    "    score = score / len(runs)\n",
    "    \n",
    "    return {\"key\": \"avg diff from true score\", \"score\": score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45825fb2-8b5e-40e9-a6ef-611a2afc3b86",
   "metadata": {},
   "source": [
    "## Full pipeline for evaluating llm evaluator for context relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "342a6c4e-2a40-422a-a455-c3f8afc76af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Prompt_testing-1528805c' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/4c17fe49-6797-40aa-a5fc-36f4809034f5/compare?selectedSessions=2860e1db-aa6d-4b0b-8de1-76d3f8ade962\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90d59b8d4f5d4f2c835a037874ac1172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Prompt_testing-1528805c' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/4c17fe49-6797-40aa-a5fc-36f4809034f5/compare?selectedSessions=2860e1db-aa6d-4b0b-8de1-76d3f8ade962\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c607a9d8b0549ab9f672fac6f68af73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults Prompt_testing-1528805c>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate eval scores\n",
    "exp = evaluate(\n",
    "    lambda x:x,\n",
    "    data=\"prompt_test_GAS\",\n",
    "    evaluators=[grade_docs],\n",
    "    experiment_prefix=\"Prompt_testing\",\n",
    ")\n",
    "\n",
    "# generate eval score for the evaluator\n",
    "evaluate_existing(exp.experiment_name, summary_evaluators=[avg_diff])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0fcfa-69bb-4708-b91b-d946e05e2fdb",
   "metadata": {},
   "source": [
    "# Ground Truth Prompt Experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d787d3d-9985-4076-8f85-bc9392d562f1",
   "metadata": {},
   "source": [
    "## Experimental LLM Evaluator being tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179aa6bb-8c2b-494f-a204-cf26d741d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundTruth(BaseModel):\n",
    "    \"\"\"List facts in handout not based on ground truth\"\"\"\n",
    "    list_of_false: List[str] = Field(description=\"List of facts in handout not based on ground truth\")\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GroundTruth)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"\n",
    "You are an expert assessor tasked with evaluating whether the generated text (provided by the user between the XML tags GENERATED TEXT) is factually based on the provided context (provided by the user between the XML tags CONTEXT).  \n",
    "\n",
    "Follow these steps:\n",
    "    Step 1: Read the provided context carefully. Understand the information presented in the context.\n",
    "    Step 2: Analyze each sentence in the GENERATED TEXT. Compare it with the provided CONTEXT to determine its factual basis. Sentences in the GENERATED TEXT that are similar (but not verbatim) to the sentences in provided CONTEXT, but are still factually aligned are considered factually based on the CONTEXT.\n",
    "    Step 3: Identify sentences in the GENERATED TEXT that are not factually based on the context (not factually supported by the provided CONTEXT or directly contradicts the provided CONTEXT.) \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "<GENERATED TEXT>\n",
    "{handout} \n",
    "</GENERATED TEXT>\n",
    "\n",
    "<CONTEXT>\n",
    "{contexts}\n",
    "</CONTEXT>\n",
    "\"\"\"\n",
    "\n",
    "prompt_groundtruth = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "grader_groundtruth = prompt_groundtruth | structured_llm_grader\n",
    "\n",
    "def grade_groundtruth(run, example):\n",
    "    try: # try to get outputs from run, otherwise it is from dataset\n",
    "        handout = run.outputs[\"handout\"]\n",
    "        contexts = run.outputs[\"contexts\"]\n",
    "    except KeyError:\n",
    "        handout = example.outputs[\"handout\"]\n",
    "        contexts = example.inputs[\"contexts\"]\n",
    "        \n",
    "    result = grader_groundtruth.invoke({\"handout\": handout, \"contexts\": contexts}).list_of_false\n",
    "    count = len(result)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"count\", \"score\": count, \"comment\": \"num infactual sentences\",\n",
    "        \"sentences\": result\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17a370-1f08-4f40-9203-17cd491e927e",
   "metadata": {},
   "source": [
    "## Metrics for evaluating the LLM evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84ebf856-aa66-48b6-a281-c2885036988b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# average difference of generated count vs ground truth count\n",
    "def avg_diff_count(runs: list[Run], examples: list[Example]) -> dict:\n",
    "    score = 0\n",
    "    for i, (run, example) in enumerate(zip(runs, examples)):\n",
    "        score += run.feedback_stats[\"count\"][\"avg\"] - int(example.outputs[\"count\"])\n",
    "\n",
    "    score = score / len(runs)\n",
    "    \n",
    "    return {\"key\": \"avg diff from true count\", \"score\": score}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aedf16b-8e0b-4bfc-9624-36cfa64e123b",
   "metadata": {},
   "source": [
    "## Full pipeline for evaluating llm evaluator for context relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec70074b-71d5-4ded-9232-10032624bfcd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grade_groundtruth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# generate eval scores\u001b[39;00m\n\u001b[1;32m      2\u001b[0m exp \u001b[38;5;241m=\u001b[39m evaluate(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x:x,\n\u001b[1;32m      4\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt_test_groundtruth\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m----> 5\u001b[0m     evaluators\u001b[38;5;241m=\u001b[39m[\u001b[43mgrade_groundtruth\u001b[49m],\n\u001b[1;32m      6\u001b[0m     experiment_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt_testing\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# generate eval score for the evaluator\u001b[39;00m\n\u001b[1;32m     11\u001b[0m evaluate_existing(exp\u001b[38;5;241m.\u001b[39mexperiment_name, summary_evaluators\u001b[38;5;241m=\u001b[39m[avg_diff_count])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grade_groundtruth' is not defined"
     ]
    }
   ],
   "source": [
    "# generate eval scores\n",
    "exp = evaluate(\n",
    "    lambda x:x,\n",
    "    data=\"Prompt_test_groundtruth\",\n",
    "    evaluators=[grade_groundtruth],\n",
    "    experiment_prefix=\"Prompt_testing\",\n",
    ")\n",
    "\n",
    "\n",
    "# generate eval score for the evaluator\n",
    "evaluate_existing(exp.experiment_name, summary_evaluators=[avg_diff_count])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ad820-5e84-408b-9fb6-82c3aaa07a39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
