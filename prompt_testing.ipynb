{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19c6c73-db89-4bb3-a655-443cbab8056b",
   "metadata": {},
   "source": [
    "# Last left off\n",
    "- how to give structured input to llm to improve accuracy\n",
    "- few prompt strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca9268-038b-48ce-9e07-528b5ecf8782",
   "metadata": {},
   "source": [
    "# Context Relevance Prompt Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0533c46a-a040-4f78-a962-a9109fef5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import _global\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langsmith.evaluation import evaluate, evaluate_existing\n",
    "from langsmith.schemas import Example, Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e707a71-dbb4-4389-82f6-cb1853f28e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\" Pydantic object used to format LLM output\n",
    "    * 0: irrelevant diagnosis \\n\n",
    "    * 1: correct diagnosis, but does not contain information to anser the user question \\n\n",
    "    * 2: correct diagnosis and contains information to answer the user question). \\n    \n",
    "    \"\"\"\n",
    "    score: int = Field(description=\"Documents grade based on correct diagnosis and relevant information\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd64fe4-bb44-4b0a-b18a-222d50d46626",
   "metadata": {},
   "source": [
    "## Experimental LLM Evaluator being tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8dbbbfe-b2ce-4d87-ab14-630a0b2d4c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a grader assessing the relevance of a retrieved document content to a query. \\n\n",
    "The query is a question about a medical diagnosis. \\n\n",
    "The document is a Python dictionary. The content of the document is under the \"page_content\" key in the dictionary. \\n\n",
    "Give a score for the document using the scoring system below. \\n\n",
    "\n",
    "# Scoring\n",
    "- **0**: The content does not contain information about the queried diagnosis \\n\n",
    "- **1**: The content contains information about the queried diagnosis, but the information does not answer the query \\n\n",
    "- **2**: The content contains information about the queried diagnosis and the information answers the query). \\n\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "# RETRIEVED DOCUMENT\n",
    "{document} \n",
    "\n",
    "# QUERY\n",
    "{query}\n",
    "\"\"\"\n",
    "prompt_gradedoc = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt_gradedoc | structured_llm_grader\n",
    "\n",
    "def grade_docs(run, example) -> dict:\n",
    "    \"\"\"Grades all queries and corresponding documents in the db. used in eval type 1\"\"\"\n",
    "    grade = retrieval_grader.invoke({\"query\": example.inputs[\"query\"], \"document\": example.inputs[\"context\"]})\n",
    "    return {\"key\": \"grade\", \"score\": int(grade.score), \"comment\": \"grade for doc\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96be70f-1ba0-43ac-8db7-eb6a2976d206",
   "metadata": {},
   "source": [
    "## Metrics for evaluating the LLM evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ba5b95-0e9d-42a6-94d8-dc7857082a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare generated eval scores with ground truth eval scores\n",
    "def summary_eval(runs: list[Run], examples: list[Example]) -> dict:\n",
    "    score = 0\n",
    "    for i, (run, example) in enumerate(zip(runs, examples)):\n",
    "        score += run.feedback_stats[\"grade\"][\"avg\"] - int(example.outputs[\"score\"])\n",
    "\n",
    "    score = score / len(runs)\n",
    "    \n",
    "    return {\"key\": \"avg diff from true score\", \"score\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dd1768c-ffaf-489e-bc4a-f07108328de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Prompt_testing-eccc400e' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/4c17fe49-6797-40aa-a5fc-36f4809034f5/compare?selectedSessions=8184c2d8-0f13-4f7c-9142-15400e1a1557\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "578b899a36634a8496e630f03a7117e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<ExperimentResults Prompt_testing-eccc400e>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_name = \"Prompt_testing-eccc400e\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45825fb2-8b5e-40e9-a6ef-611a2afc3b86",
   "metadata": {},
   "source": [
    "## Full pipeline for evaluating llm evaluator for context relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a6c4e-2a40-422a-a455-c3f8afc76af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate eval scores\n",
    "exp = evaluate(\n",
    "    lambda x:x,\n",
    "    data=\"prompt_test_GAS\",\n",
    "    evaluators=[grade_docs],\n",
    "    experiment_prefix=\"Prompt_testing\",\n",
    ")\n",
    "\n",
    "# generate eval score for the evaluator\n",
    "evaluate_existing(exp.experiment_name, summary_evaluators=[summary_eval])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0fcfa-69bb-4708-b91b-d946e05e2fdb",
   "metadata": {},
   "source": [
    "# Ground Truth Prompt Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179aa6bb-8c2b-494f-a204-cf26d741d74d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
