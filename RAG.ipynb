{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa6c287f-3b06-4b73-8b54-87a893580a60",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaac6de6-2ef6-4586-9184-8d93a3990363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from _global import path_to_resources, hf_embed\n",
    "import templates\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "import langsmith\n",
    "from langsmith import traceable, trace\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "from langchain.callbacks.tracers import LangChainTracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca634257-130b-4d4b-8f7f-0e8425dd5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up retriever\n",
    "db = Chroma(collection_name=\"main_collection\", persist_directory=f\"{path_to_resources}/db_wiki\", embedding_function=hf_embed)\n",
    "retriever = db.as_retriever(\n",
    "                search_type = \"similarity\",\n",
    "                search_kwargs = {\"k\":4},\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6392a02-2836-4da9-b7c1-8c4193b6952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langsmith setup\n",
    "project_name = \"ED-handout\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732bba5-125c-43c1-94b3-5f35c6158a0f",
   "metadata": {},
   "source": [
    "# RAG Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b228d657-be08-4d01-94af-fa08b78d3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagBot:\n",
    "    def __init__(self, retriever, templates, model: str = \"gpt-3.5-turbo-1106\"):\n",
    "        self._retriever = retriever\n",
    "        self._llm_gpt = ChatOpenAI(model_name=model, temperature=0)\n",
    "        self._llm_llama = Ollama(model=\"llama2:13b\", temperature=0)\n",
    "        self.templates = templates\n",
    "        self.queries = {\n",
    "            \"definition\": \"definition of {diagnosis}\",\n",
    "            \"presentation\": \"manifestations of {diagnosis}\",\n",
    "            \"course\": \"natural history of {diagnosis}\",\n",
    "            \"management\": \"treatment and management for {diagnosis}\",\n",
    "            \"follow_up\": \"follow-up plan for {diagnosis}\",\n",
    "            \"redflags\": \"signs and symptoms that indicate the need for urgent medical attention for patients with {diagnosis}\",\n",
    "        }\n",
    "\n",
    "    \n",
    "    @traceable\n",
    "    def diagnosis_extraction(self, assessment):\n",
    "        \"\"\"Extracts diagnosis from physician's assessment of the patient\"\"\"\n",
    "        prompt_extract_diagnosis = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",self.templates.extract_diagnosis_system),\n",
    "            (\"human\", \"{assessment}\")\n",
    "        ])\n",
    "        chain_diagnosis = prompt_extract_diagnosis | self._llm_gpt\n",
    "        \n",
    "        return chain_diagnosis.invoke({\"assessment\":assessment}).content\n",
    "\n",
    "    \n",
    "    def make_queries(self, diagnosis):\n",
    "        \"\"\"Uses the diagnosis to populate dict of queries that will be used to retreive context from db\"\"\"\n",
    "        return {key: value.format(diagnosis=diagnosis) for key, value in self.queries.items()}\n",
    "\n",
    "    \n",
    "    @traceable(run_type=\"retriever\")\n",
    "    def _retrieve_docs(self, query):\n",
    "        return self._retriever.invoke(query)\n",
    "\n",
    "    \n",
    "    def get_contexts(self, queries):\n",
    "        \"\"\"returns a tuple with (query, contexts)\"\"\"\n",
    "        contexts = {}\n",
    "        for k, query in queries.items():\n",
    "            contexts[k] = (query, self._retrieve_docs(query))\n",
    "        \n",
    "        return contexts\n",
    "\n",
    "\n",
    "    def compress_contexts(self, q_c):\n",
    "        prompt_compress = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.templates.compress_context_system),\n",
    "            (\"human\", self.templates.compress_context_human)\n",
    "        ])\n",
    "        chain_compress = prompt_compress | self._llm_gpt\n",
    "\n",
    "        return chain_compress.invoke({\"query\": q_c[0], \"context\": q_c[1]}).content\n",
    "\n",
    "    \n",
    "    @traceable()\n",
    "    def retrieval_steps(self, assessment):\n",
    "        \"\"\"All the steps to prep the contexts for final handout generation\"\"\"    \n",
    "        diagnosis = self.diagnosis_extraction(assessment)\n",
    "        queries = self.make_queries(diagnosis)\n",
    "        contexts = self.get_contexts(queries)\n",
    "\n",
    "        return {\"contexts\": contexts, \"diagnosis\": diagnosis}\n",
    "        \n",
    "    \n",
    "    @traceable()\n",
    "    def make_handout(self, assessment, md_plan):\n",
    "        _run_input = self.retrieval_steps(assessment)\n",
    "        _contexts = _run_input[\"contexts\"]\n",
    "        diagnosis = _run_input[\"diagnosis\"]\n",
    "\n",
    "        # compression\n",
    "        contexts = {}\n",
    "        for k, q_c in _contexts.items():\n",
    "            contexts[k] = self.compress_contexts(q_c)\n",
    "\n",
    "        # make handout\n",
    "        prompt_make_handout = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",self.templates.handout_generation_system),\n",
    "            (\"human\", self.templates.handout_generation_human),\n",
    "        ])\n",
    "        chain_make_handout = prompt_make_handout | self._llm_gpt\n",
    "        response = chain_make_handout.invoke({\n",
    "            \"context_definition\": contexts[\"definition\"],\n",
    "            \"context_presentation\": contexts[\"presentation\"],\n",
    "            \"context_course\": contexts[\"course\"],\n",
    "            \"context_management\": contexts[\"management\"],\n",
    "            \"context_follow_up\": contexts[\"follow_up\"],\n",
    "            \"context_redflags\": contexts[\"redflags\"],\n",
    "            \"context_md_plan\": md_plan,\n",
    "        })\n",
    "        \n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            \"diagnosis\": diagnosis,\n",
    "            \"contexts\": \"\\n\".join(contexts.values()),\n",
    "            \"handout\": response.content,\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98b66bff-0d41-4fbd-bed3-3eac47d5c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_handout_with_context(example: dict):\n",
    "    \"\"\"Use this for evaluation of retrieved documents and hallucinations\"\"\"\n",
    "    response = rag_bot.get_answer(example[\"question\"])\n",
    "    print(response)\n",
    "    return {\"handout\": response[\"handout\"], \"contexts\": response[\"contexts\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f810ed-9511-4576-b64d-67b34f25a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = RagBot(retriever, templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffc37dd4-127c-4c5f-96db-89c1e8b85b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that extraction works and works with langsmith\n",
    "\n",
    "with trace(\"Diagnosis extraction\", \"chain\", project_name=project_name, inputs={\"assessment\": \"5yo M with viral-triggered asthma\"}) as rt:\n",
    "    output = bot.diagnosis_extraction(inputs[\"assessment\"])\n",
    "    rt.end(outputs={\"output\": output})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55ef3f-50cd-4157-8f41-dac745b4cbde",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481bf51-268e-4417-adf2-c1ae7bc42c06",
   "metadata": {},
   "source": [
    "## Doc grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6887816a-7680-4cef-b0e5-a9fe19d2d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OpenAI Grader\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    score: str = Field(description=\"Documents grade based on correct diagnosis and relevant information\")\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    The content of the document can be found in page_content. Give a score for the document using the scoring system below. \n",
    "    Scoring: (0: irrelevant diagnosis), (1: correct diagnosis, but does not contain information to anser the user question), (2: correct diagnosis and contains information to answer the user question). \\n\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"RETRIEVED DOCUMENT: \\n\\n {document} \\n\\n USER QUESTION: {query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader_oai = grade_prompt | structured_llm_grader\n",
    "\n",
    "def predict_oai(inputs: dict) -> dict:\n",
    "    # Returns pydantic object\n",
    "    grade = retrieval_grader_oai.invoke({\"query\": inputs[\"query\"], \"document\": inputs[\"context\"]})\n",
    "    return {\"grade\":grade.score}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7769c3-2cc1-4bc9-96a6-078442e079c6",
   "metadata": {},
   "source": [
    "- given a diagnosis\n",
    "- create dataset of query + doc for each doc retrieved from each query\n",
    "- run experiement on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "479b7eb9-786a-4d54-b30f-b624f7cf5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(diagnosis, context_dict, dataset_name):\n",
    "    \"\"\"Takes query_context dictionary and create a dataset for {diagnosis} to evaluate the relevance of retrieved context\"\"\"\n",
    "    client = langsmith.Client()\n",
    "    \n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=f\"Test context relevance for docs retreiived for {diagnosis}\",\n",
    "    )\n",
    "\n",
    "    for query, q_c in context_dict.values(): #each document should be an example in the dataset        \n",
    "        for doc in q_c:\n",
    "            client.create_examples(\n",
    "                inputs=[{\"query\": query, \"context\": doc}],\n",
    "                dataset_id=dataset.id,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f2d24128-e746-48bd-8ec9-e3e730a23097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def context_relevance(rag_bot, assessment):\n",
    "    retrieved = rag_bot.retrieval_steps(assessment) # dict of query:context\n",
    "    context_dict = retrieved[\"contexts\"]\n",
    "    diagnosis = retrieved[\"diagnosis\"]\n",
    "\n",
    "    dataset_name = f\"Queries_Docs_{diagnosis}\"\n",
    "    create_dataset(diagnosis, context_dict, dataset_name)\n",
    "        \n",
    "    evaluate(\n",
    "        predict_oai,\n",
    "        data=dataset_name,\n",
    "        #summary_evaluators=[f1_score_summary_evaluator],\n",
    "        experiment_prefix=\"Context-relevance-\",\n",
    "        # Any experiment metadata can be specified here\n",
    "        metadata={\n",
    "            \"model\": \"oai\",\n",
    "            \"diagnosis\":diagnosis\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c33348ea-1008-4e52-a7fb-6d4ff9882bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Context-relevance--d321b186' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/2c62c230-5ffb-4be1-a160-67b532c3d537/compare?selectedSessions=4a781120-8ba7-45ee-ac5f-031e0c2ddd8d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c700faa3b14924b63e99d989060f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_relevance(bot, \"5yo M, viral triggered asthma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffa5be-acaa-4a9e-bb05-456b47c18b9b",
   "metadata": {},
   "source": [
    "## Ground truth checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a438330-7ffe-4aa6-88ec-16b51f1255cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "answer_hallucination_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"\"\"Is the Assistant's Answer grounded in the Ground Truth documentation? A score of [[1]] means that the\n",
    "            Assistant answer contains is not at all based upon / grounded in the Groun Truth documentation. A score of [[5]] means \n",
    "            that the Assistant answer contains some information (e.g., a hallucination) that is not captured in the Ground Truth \n",
    "            documentation. A score of [[10]] means that the Assistant answer is fully based upon the in the Ground Truth documentation.\"\"\"\n",
    "        },\n",
    "        # If you want the score to be saved on a scale from 0 to 1\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"handout\"],\n",
    "        \"reference\": run.outputs[\"contexts\"],\n",
    "        \"input\": example.inputs,\n",
    "    },\n",
    ")\n",
    "\n",
    "dataset_name = \"RAG_test_LCEL\"\n",
    "experiment_results = evaluate(\n",
    "    predict_rag_answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-qa-oai-hallucination\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"LCEL context, gpt-3.5-turbo\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc3492-d5e1-426e-9428-01d2083cd48e",
   "metadata": {},
   "source": [
    "## LLM grading based on custom metrics\n",
    "- jargon\n",
    "- reference list\n",
    "- template format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a519e-40f6-4cfd-b9f9-27cf48252fa7",
   "metadata": {},
   "source": [
    "## Human feedback of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593e441-f1d7-4521-96a2-ca060923a33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
