{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7d071a-daf7-40dc-a652-860a71a06fd8",
   "metadata": {},
   "source": [
    "# Task List\n",
    "- GPT3.5 LLM evaluator for grading context relevance is inaccurate\n",
    "- was cleaning up code and adding comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6c287f-3b06-4b73-8b54-87a893580a60",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaac6de6-2ef6-4586-9184-8d93a3990363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "from _global import path_to_resources, hf_embed\n",
    "import templates\n",
    "\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import langsmith\n",
    "from langsmith import traceable, trace\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca634257-130b-4d4b-8f7f-0e8425dd5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up retriever\n",
    "db = Chroma(collection_name=\"main_collection\", persist_directory=f\"{path_to_resources}/db_main\", embedding_function=hf_embed)\n",
    "retriever = db.as_retriever(\n",
    "                search_type = \"similarity\",\n",
    "                search_kwargs = {\"k\":4},\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6392a02-2836-4da9-b7c1-8c4193b6952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langsmith setup\n",
    "project_name = \"ED-handout\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732bba5-125c-43c1-94b3-5f35c6158a0f",
   "metadata": {},
   "source": [
    "# RAG Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b228d657-be08-4d01-94af-fa08b78d3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagBot:\n",
    "    \"\"\"Bot that handles different steps of RAG\"\"\"\n",
    "    def __init__(self, retriever, templates, model: str = \"gpt-3.5-turbo-1106\"):\n",
    "        self._retriever = retriever\n",
    "        self._llm_gpt = ChatOpenAI(model_name=model, temperature=0)\n",
    "        self._llm_llama = Ollama(model=\"llama2:13b\", temperature=0)\n",
    "        self.templates = templates\n",
    "        self.queries = {\n",
    "            \"definition\": \"definition of {diagnosis}\",\n",
    "            \"presentation\": \"manifestations of {diagnosis}\",\n",
    "            \"course\": \"natural history of {diagnosis}\",\n",
    "            \"management\": \"treatment and management for {diagnosis}\",\n",
    "            \"follow_up\": \"follow-up plan for {diagnosis}\",\n",
    "            \"redflags\": \"signs and symptoms that indicate the need for urgent medical attention for patients with {diagnosis}\",\n",
    "        }\n",
    "\n",
    "    \n",
    "    @traceable\n",
    "    def diagnosis_extraction(self, assessment):\n",
    "        \"\"\"Extracts diagnosis from physician's assessment of the patient\"\"\"\n",
    "        prompt_extract_diagnosis = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",self.templates.extract_diagnosis_system),\n",
    "            (\"human\", \"{assessment}\")\n",
    "        ])\n",
    "        chain_diagnosis = prompt_extract_diagnosis | self._llm_gpt\n",
    "        \n",
    "        return chain_diagnosis.invoke({\"assessment\":assessment}).content\n",
    "\n",
    "    \n",
    "    def make_queries(self, diagnosis):\n",
    "        \"\"\"Uses the diagnosis to populate dict of queries that will be used to retreive context from db\"\"\"\n",
    "        return {key: value.format(diagnosis=diagnosis) for key, value in self.queries.items()}\n",
    "\n",
    "    \n",
    "    @traceable(run_type=\"retriever\")\n",
    "    def _retrieve_docs(self, query):\n",
    "        return self._retriever.invoke(query)\n",
    "\n",
    "    \n",
    "    def get_contexts(self, queries):\n",
    "        \"\"\"returns dict with tuples of (query, contexts)\"\"\"\n",
    "        contexts = {}\n",
    "        for k, query in queries.items():\n",
    "            contexts[k] = (query, self._retrieve_docs(query))\n",
    "        \n",
    "        return contexts\n",
    "\n",
    "\n",
    "    def compress_contexts(self, q_c):\n",
    "        \"\"\"contextual compression with llm\"\"\"\n",
    "        prompt_compress = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.templates.compress_context_system),\n",
    "            (\"human\", self.templates.compress_context_human)\n",
    "        ])\n",
    "        chain_compress = prompt_compress | self._llm_gpt\n",
    "\n",
    "        return chain_compress.invoke({\"query\": q_c[0], \"context\": q_c[1]}).content\n",
    "\n",
    "    \n",
    "    @traceable()\n",
    "    def retrieval_steps(self, assessment, eval=False):\n",
    "        \"\"\"all the steps to prep the contexts for final handout generation\"\"\"    \n",
    "        diagnosis = self.diagnosis_extraction(assessment)\n",
    "        queries = self.make_queries(diagnosis)\n",
    "        contexts = self.get_contexts(queries)\n",
    "\n",
    "        return {\"contexts\": contexts, \"diagnosis\": diagnosis}\n",
    "        \n",
    "    \n",
    "    @traceable()\n",
    "    def make_handout(self, assessment, md_plan):\n",
    "        _run_input = self.retrieval_steps(assessment)\n",
    "        _contexts = _run_input[\"contexts\"]\n",
    "        diagnosis = _run_input[\"diagnosis\"]\n",
    "\n",
    "        # compression\n",
    "        contexts = {}\n",
    "        for k, q_c in _contexts.items():\n",
    "            contexts[k] = self.compress_contexts(q_c)\n",
    "\n",
    "        # make handout\n",
    "        prompt_make_handout = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",self.templates.handout_generation_system),\n",
    "            (\"human\", self.templates.handout_generation_human),\n",
    "        ])\n",
    "        chain_make_handout = prompt_make_handout | self._llm_gpt\n",
    "        response = chain_make_handout.invoke({\n",
    "            \"context_definition\": contexts[\"definition\"],\n",
    "            \"context_presentation\": contexts[\"presentation\"],\n",
    "            \"context_course\": contexts[\"course\"],\n",
    "            \"context_management\": contexts[\"management\"],\n",
    "            \"context_follow_up\": contexts[\"follow_up\"],\n",
    "            \"context_redflags\": contexts[\"redflags\"],\n",
    "            \"context_md_plan\": md_plan,\n",
    "        })\n",
    "        \n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            \"diagnosis\": diagnosis,\n",
    "            \"contexts\": \"\\n\".join(contexts.values()) + \"\\n\" + md_plan,\n",
    "            \"handout\": response.content\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f810ed-9511-4576-b64d-67b34f25a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_bot = RagBot(retriever, templates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55ef3f-50cd-4157-8f41-dac745b4cbde",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fd4e5-11e3-437c-8c28-21ce3e3c2465",
   "metadata": {},
   "source": [
    "ways to use evaluations\n",
    "- to evaluate part of the RAG pipeline\n",
    "    1. **before handout generation**: this runs part of the rag pipeline -> create a dataset -> evaluate based on the data; used when optimizing each section (e.g prompt engineering, experimenting with retrieval strategies)\n",
    "    2. **after handout generation**: run on one example (dataset with assessment and plan as input for one diagnosis); used to test debug for individual case\n",
    "- evaluate the whole pipeline: run for all common diagnoses dataset (Pt_cases); evaluate the RAG chain on a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481bf51-268e-4417-adf2-c1ae7bc42c06",
   "metadata": {},
   "source": [
    "## Doc grader\n",
    "- given a diagnosis, create dataset of query + doc for each doc retrieved from each query\n",
    "- run experiement on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6887816a-7680-4cef-b0e5-a9fe19d2d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\" Pydantic object used to format LLM output\n",
    "    * 0: irrelevant diagnosis \\n\n",
    "    * 1: correct diagnosis, but does not contain information to anser the user question \\n\n",
    "    * 2: correct diagnosis and contains information to answer the user question). \\n    \n",
    "    \"\"\"\n",
    "    score: str = Field(description=\"Documents grade based on correct diagnosis and relevant information\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    The content of the document can be found in page_content. Give a score for the document using the scoring system below. \n",
    "    Scoring: \n",
    "    * 0: irrelevant diagnosis \\n\n",
    "    * 1: correct diagnosis, but does not contain information to anser the user question \\n\n",
    "    * 2: correct diagnosis and contains information to answer the user question). \\n\n",
    "\"\"\"\n",
    "prompt_gradedoc = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"RETRIEVED DOCUMENT: \\n\\n {document} \\n\\n USER QUESTION: {query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt_gradedoc | structured_llm_grader\n",
    "\n",
    "def grade_doc(query, doc) -> dict:\n",
    "    \"\"\"Grades one query and one corresponding document. used in eval type 3\"\"\"\n",
    "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
    "    return {\"key\": \"grade\", \"score\": int(grade.score), \"comment\": \"grade for doc\"}\n",
    "\n",
    "def grade_docs(run, example) -> dict:\n",
    "    \"\"\"Grades all queries and corresponding documents in the db. used in eval type 1\"\"\"\n",
    "    grade = retrieval_grader.invoke({\"query\": example.inputs[\"query\"], \"document\": example.inputs[\"context\"]})\n",
    "    return {\"key\": \"grade\", \"score\": int(grade.score), \"comment\": \"grade for doc\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "479b7eb9-786a-4d54-b30f-b624f7cf5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_relevance(diagnosis, context_dict, dataset_name):\n",
    "    \"\"\"Takes query_context dictionary and create a dataset for {diagnosis} to evaluate the relevance of retrieved context\"\"\"\n",
    "    client = langsmith.Client()\n",
    "    \n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=f\"Test context relevance for docs retreiived for {diagnosis}\",\n",
    "    )\n",
    "\n",
    "    for query, q_c in context_dict.values(): #each document should be an example in the dataset        \n",
    "        for doc in q_c:\n",
    "            client.create_examples(\n",
    "                inputs=[{\"query\": query, \"context\": doc}],\n",
    "                dataset_id=dataset.id,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2d24128-e746-48bd-8ec9-e3e730a23097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval type: 1; used to asses the retrieved context's relevance\n",
    "\n",
    "def eval_context_relevance(rag_bot, assessment):\n",
    "    retrieved = rag_bot.retrieval_steps(assessment) # dict of query:context\n",
    "    context_dict = retrieved[\"contexts\"]\n",
    "    diagnosis = retrieved[\"diagnosis\"]\n",
    "\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d, %H:%M:%S')\n",
    "    dataset_name = f\"Queries_Docs_{diagnosis}_{current_time}\"\n",
    "    create_dataset_relevance(diagnosis, context_dict, dataset_name)\n",
    "        \n",
    "    evaluate(\n",
    "        lambda x:x,\n",
    "        data=dataset_name,\n",
    "        evaluators=[grade_docs],\n",
    "        experiment_prefix=\"Context-relevance-\",\n",
    "        metadata={\n",
    "            \"model\": \"oai\",\n",
    "            \"diagnosis\":diagnosis\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c33348ea-1008-4e52-a7fb-6d4ff9882bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Context-relevance--a75c23db' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/849d8b2c-52d8-40de-99d6-f414a23ae3f6/compare?selectedSessions=9c656256-5590-470c-904a-f0217f28748f\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0053edf887d4b74905e042990f190e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_context_relevance(rag_bot, \"5yo M, viral triggered asthma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffa5be-acaa-4a9e-bb05-456b47c18b9b",
   "metadata": {},
   "source": [
    "## Faithful checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b4ec3cb-0292-4f9b-98a7-61f7713650e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in use; used once to create dataset for eval_faithfulness_evaluator_prompt\n",
    "def create_dataset_faithfulness(handout, context, dataset_name):\n",
    "    \"\"\"Takes handout and contexts used to make a dataset\"\"\"\n",
    "    client = langsmith.Client()\n",
    "    \n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=f\"Test whether handout for {diagnosis} is based on provided context\",\n",
    "    )\n",
    "\n",
    "    # **Preprocess context to remove headings\n",
    "    client.create_examples(\n",
    "        inputs=[{\"contexts\": contexts}],\n",
    "        outputs=[{\"handout\": handout}],\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a5989e-1058-42d9-a678-754fec521738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2a8f8bc-5f7e-49a6-a636-598a2694ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Faithfulness(BaseModel):\n",
    "    \"\"\"List facts in handout not based on ground truth\"\"\"\n",
    "    list_of_false: List[str] = Field(description=\"List of facts in handout not based on ground truth\")\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(Faithfulness)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"\n",
    "You are an expert assessor tasked with evaluating whether the generated text (provided by the user between the XML tags GENERATED TEXT) is factually based on the provided context (provided by the user between the XML tags CONTEXT).  \n",
    "\n",
    "Follow these steps:\n",
    "    Step 1: Read the provided context carefully. Understand the information presented in the context.\n",
    "    Step 2: Analyze each sentence in the GENERATED TEXT. Compare it with the provided CONTEXT to determine its factual basis. Sentences in the GENERATED TEXT that are similar (but not verbatim) to the sentences in provided CONTEXT, but are still factually aligned are considered factually based on the CONTEXT.\n",
    "    Step 3: Identify sentences in the GENERATED TEXT that are not factually based on the context (not factually supported by the provided CONTEXT or directly contradicts the provided CONTEXT.) \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "<GENERATED TEXT>\n",
    "{handout} \n",
    "</GENERATED TEXT>\n",
    "\n",
    "<CONTEXT>\n",
    "{contexts}\n",
    "</CONTEXT>\n",
    "\"\"\"\n",
    "\n",
    "prompt_faithfulness= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "grader_faithfulness= prompt_faithfulness| structured_llm_grader\n",
    "\n",
    "def grade_faithfulness(run, example):\n",
    "    try: # try to get outputs from run, otherwise it is from dataset\n",
    "        handout = run.outputs[\"handout\"]\n",
    "        contexts = run.outputs[\"contexts\"]\n",
    "    except KeyError:\n",
    "        handout = example.outputs[\"handout\"]\n",
    "        contexts = example.inputs[\"contexts\"]\n",
    "        \n",
    "    result = grader_faithfulness.invoke({\"handout\": handout, \"contexts\": contexts}).list_of_false\n",
    "    count = len(result)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"count\", \"score\": count, \"comment\": \"num infactual sentences\",\n",
    "        \"sentences\": result\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cc83451-f127-4d4f-aca1-f4145f92f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval type: 1; to experiment with prompt engineering for LLM evaluator using dataset on context and handout\n",
    "\n",
    "def eval_faithfulness_evaluator_prompt(rag_bot):\n",
    "    \"\"\"to evaluate prompt for LLM assessor for faithfulness evaluation task\"\"\"\n",
    "    dataset_name = \"Context_faithfulness\"\n",
    "        \n",
    "    evaluate(\n",
    "        lambda x:x,\n",
    "        data=dataset_name,\n",
    "        evaluators=[grade_faithfulness],\n",
    "        experiment_prefix=\"Faithfulness-prompt-\",\n",
    "        metadata={\n",
    "            \"model\": \"oai\",\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b130477-99a2-4507-8300-0201207474d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "LangSmithNotFoundError",
     "evalue": "Dataset Context_faithfulness not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLangSmithNotFoundError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_faithfulness_evaluator_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrag_bot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 7\u001b[0m, in \u001b[0;36meval_faithfulness_evaluator_prompt\u001b[0;34m(rag_bot)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"to evaluate prompt for LLM assessor for faithfulness evaluation task\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext_faithfulness\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mgrade_faithfulness\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mFaithfulness-prompt-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py:235\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, client, blocking)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m     76\u001b[0m     target: TARGET_T,\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ExperimentResults:\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate a target system or function on a given dataset.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m        View the evaluation results for experiment:...\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py:791\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, client, blocking, experiment)\u001b[0m\n\u001b[1;32m    775\u001b[0m runs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m _is_callable(target) \u001b[38;5;28;01melse\u001b[39;00m cast(Iterable[schemas\u001b[38;5;241m.\u001b[39mRun], target)\n\u001b[1;32m    776\u001b[0m experiment_, runs \u001b[38;5;241m=\u001b[39m _resolve_experiment(\n\u001b[1;32m    777\u001b[0m     experiment,\n\u001b[1;32m    778\u001b[0m     runs,\n\u001b[1;32m    779\u001b[0m     client,\n\u001b[1;32m    780\u001b[0m )\n\u001b[1;32m    782\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43m_ExperimentManager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If provided, we don't need to create a new experiment.\u001b[39;49;00m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mruns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mruns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Create or resolve the experiment.\u001b[39;49;00m\n\u001b[0;32m--> 791\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    792\u001b[0m cache_dir \u001b[38;5;241m=\u001b[39m ls_utils\u001b[38;5;241m.\u001b[39mget_cache_dir(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    793\u001b[0m cache_path \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    794\u001b[0m     pathlib\u001b[38;5;241m.\u001b[39mPath(cache_dir) \u001b[38;5;241m/\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmanager\u001b[38;5;241m.\u001b[39mdataset_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cache_dir \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    795\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py:1056\u001b[0m, in \u001b[0;36m_ExperimentManager.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstart\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ExperimentManager:\n\u001b[0;32m-> 1056\u001b[0m     first_example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m     project \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_project(first_example)\n\u001b[1;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_experiment_start(project, first_example)\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/client.py:3146\u001b[0m, in \u001b[0;36mClient.list_examples\u001b[0;34m(self, dataset_id, dataset_name, example_ids, as_of, splits, inline_s3_urls, limit, metadata, **kwargs)\u001b[0m\n\u001b[1;32m   3144\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset_id\n\u001b[1;32m   3145\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dataset_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3146\u001b[0m     dataset_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m   3147\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset_id\n\u001b[1;32m   3148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/utils.py:101\u001b[0m, in \u001b[0;36mxor_args.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     invalid_group_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(arg_groups[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m invalid_groups]\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExactly one argument in each of the following\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m groups must be defined:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(invalid_group_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     )\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/client.py:2402\u001b[0m, in \u001b[0;36mClient.read_dataset\u001b[0;34m(self, dataset_name, dataset_id)\u001b[0m\n\u001b[1;32m   2400\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m   2401\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(result) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2402\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ls_utils\u001b[38;5;241m.\u001b[39mLangSmithNotFoundError(\n\u001b[1;32m   2403\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2404\u001b[0m         )\n\u001b[1;32m   2405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m   2406\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m   2407\u001b[0m         _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[1;32m   2408\u001b[0m         _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_tenant_id(),\n\u001b[1;32m   2409\u001b[0m     )\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ls_schemas\u001b[38;5;241m.\u001b[39mDataset(\n\u001b[1;32m   2411\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mresult,\n\u001b[1;32m   2412\u001b[0m     _host_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_host_url,\n\u001b[1;32m   2413\u001b[0m     _tenant_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_optional_tenant_id(),\n\u001b[1;32m   2414\u001b[0m )\n",
      "\u001b[0;31mLangSmithNotFoundError\u001b[0m: Dataset Context_faithfulness not found"
     ]
    }
   ],
   "source": [
    "eval_faithfulness_evaluator_prompt(\n",
    "    rag_bot,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a281f1ae-a83f-4f24-86f1-7c3c57a3e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval type: 2; to evaluate context faithfulness for one diagnosis by running the whole rag chain. No dataset created in this process\n",
    "def eval_context_faithfulness(rag_bot, assessment, md_plan):\n",
    "    evaluate(\n",
    "        lambda x: rag_bot.make_handout(assessment, md_plan),\n",
    "        data=\"Faithfulness\", # dummy dataset\n",
    "        evaluators=[grade_faithfulness],\n",
    "        experiment_prefix=\"Faithfulness-\",\n",
    "        metadata={\n",
    "            \"model\": \"oai\",\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "591ea43b-4f87-4d96-b002-6ea2fd7ea73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Faithfulness--64dd2079' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/6ce351fc-f177-4f74-9322-8c8cd25579d4/compare?selectedSessions=07d6053c-5bfc-459c-9f8e-42bf08d1f644\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e20fc6e0f148219b53360caa773d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_context_faithfulness(\n",
    "    rag_bot,\n",
    "    assessment=\"5yo M, first asthma exacerbation, virally triggered\", \n",
    "    md_plan=\"-continue ventolin q4h \\n -continue flovent 125mcg qdaily \\n -follow-up with your family doctor in 2 days\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bf0de-f581-4c8b-92e0-ef0c53e562f5",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a90a08d-338a-421e-ab7a-c733dd222c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_relevance(root_run, example):\n",
    "    \"\"\"\n",
    "    A very simple evaluator that checks to see if the input of the retrieval step exists\n",
    "    in the retrieved docs.\n",
    "    \"\"\"\n",
    "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"make_handout\")\n",
    "    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieval_steps\")\n",
    "\n",
    "    context_dict = retrieve_run.outputs[\"contexts\"]\n",
    "\n",
    "    scores = []\n",
    "    for query, q_c in context_dict.values(): #each document should be an example in the dataset        \n",
    "        for doc in q_c:\n",
    "            scores.append(grade_doc(query, doc))\n",
    "            \n",
    "    return {\"results\": scores}\n",
    "    \n",
    "\n",
    "def faithfulness(root_run, example):\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks to see the answer is grounded in the documents\n",
    "    \"\"\"\n",
    "    # Get documents and answer\n",
    "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"make_handout\")\n",
    "\n",
    "    return grade_faithfulness(rag_pipeline_run, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617e5b10-8f60-4c24-90f7-222fc7ed6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_llm_allcases_1(rag_bot):\n",
    "    experiment_results = evaluate(\n",
    "        lambda inputs: rag_bot.make_handout(assessment=inputs[\"assessment\"], md_plan=inputs[\"plan\"]),\n",
    "        data=\"Dummy_pt_case\",\n",
    "        evaluators=[document_relevance, faithfulness],\n",
    "        experiment_prefix=datetime.now().strftime('%Y-%m-%d, %H:%M:%S')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee06f395-6eaf-4cad-87fd-cd5117941b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: '2024-06-20, 17:00:36-21b860ba' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/281774a1-94f9-47f1-9846-88086ceec7d5/compare?selectedSessions=0c07e3b7-890f-474b-b515-6f84833ccafb\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf6f60fffcb4d31931d611b3a6de5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_llm_allcases_1(rag_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66492c7b-42d7-4e3c-8cab-c23f0986b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_llm_allcases_2(rag_bot):\n",
    "    experiment_results = evaluate(\n",
    "        lambda inputs: rag_bot.make_handout(assessment=inputs[\"assessment\"], md_plan=inputs[\"plan\"]),\n",
    "        data=\"Pt_cases\",\n",
    "        evaluators=[document_relevance, faithfulness],\n",
    "        experiment_prefix=datetime.now().strftime('%Y-%m-%d, %H:%M:%S')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24e4f31d-e601-46f7-a6b1-f228e2729398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: '2024-06-20, 17:08:12-276d0867' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/e3957f7c-e232-4541-beef-d7216ab12241/compare?selectedSessions=37eba5ab-c749-465c-8aec-3b12e19602b3\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d0210a78e6743b584cf93bd900906ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59065, Requested 2196. Please try again in 1.261s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 58819, Requested 2079. Please try again in 898ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59645, Requested 1391. Please try again in 1.036s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59653, Requested 1769. Please try again in 1.422s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 58572, Requested 2867. Please try again in 1.439s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59251, Requested 3381. Please try again in 2.632s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running target function: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-1106 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 58917, Requested 2121. Please try again in 1.038s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator document_relevance> on run 60616e5b-0067-42d9-a2ef-3a8d99ac5eba: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59813, Requested 420. Please try again in 233ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 14, in document_relevance\n",
      "    scores.append(grade_doc(query, doc))\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/3432128128.py\", line 33, in grade_doc\n",
      "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59813, Requested 420. Please try again in 233ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator document_relevance> on run fc284fa1-6616-4683-b355-b1d522634495: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59967, Requested 574. Please try again in 541ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 14, in document_relevance\n",
      "    scores.append(grade_doc(query, doc))\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/3432128128.py\", line 33, in grade_doc\n",
      "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59967, Requested 574. Please try again in 541ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator document_relevance> on run 83198138-a3e6-420f-a3cb-b3021cb80182: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59995, Requested 550. Please try again in 545ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 14, in document_relevance\n",
      "    scores.append(grade_doc(query, doc))\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/3432128128.py\", line 33, in grade_doc\n",
      "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59995, Requested 550. Please try again in 545ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator document_relevance> on run a4e50421-f165-4014-88c2-19de53f718db: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59818, Requested 568. Please try again in 386ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 14, in document_relevance\n",
      "    scores.append(grade_doc(query, doc))\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/3432128128.py\", line 33, in grade_doc\n",
      "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59818, Requested 568. Please try again in 386ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator document_relevance> on run 8660eb4f-a7b9-444a-947a-5b5a6a72fb95: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59561, Requested 748. Please try again in 309ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 14, in document_relevance\n",
      "    scores.append(grade_doc(query, doc))\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/3432128128.py\", line 33, in grade_doc\n",
      "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59561, Requested 748. Please try again in 309ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator faithfulness> on run fc284fa1-6616-4683-b355-b1d522634495: AttributeError(\"'NoneType' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 41, in grade_faithfulness\n",
      "    handout = run.outputs[\"handout\"]\n",
      "KeyError: 'handout'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 26, in faithfulness\n",
      "    return grade_faithfulness(rag_pipeline_run, None)\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 44, in grade_faithfulness\n",
      "    handout = example.outputs[\"handout\"]\n",
      "AttributeError: 'NoneType' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator faithfulness> on run 83198138-a3e6-420f-a3cb-b3021cb80182: AttributeError(\"'NoneType' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 41, in grade_faithfulness\n",
      "    handout = run.outputs[\"handout\"]\n",
      "KeyError: 'handout'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 26, in faithfulness\n",
      "    return grade_faithfulness(rag_pipeline_run, None)\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 44, in grade_faithfulness\n",
      "    handout = example.outputs[\"handout\"]\n",
      "AttributeError: 'NoneType' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator faithfulness> on run a4e50421-f165-4014-88c2-19de53f718db: AttributeError(\"'NoneType' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 41, in grade_faithfulness\n",
      "    handout = run.outputs[\"handout\"]\n",
      "KeyError: 'handout'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 26, in faithfulness\n",
      "    return grade_faithfulness(rag_pipeline_run, None)\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 44, in grade_faithfulness\n",
      "    handout = example.outputs[\"handout\"]\n",
      "AttributeError: 'NoneType' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator document_relevance> on run 74c73f2a-249d-4d5b-b235-7923abbca97c: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59680, Requested 1235. Please try again in 915ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 14, in document_relevance\n",
      "    scores.append(grade_doc(query, doc))\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/3432128128.py\", line 33, in grade_doc\n",
      "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59680, Requested 1235. Please try again in 915ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator faithfulness> on run 8660eb4f-a7b9-444a-947a-5b5a6a72fb95: AttributeError(\"'NoneType' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 41, in grade_faithfulness\n",
      "    handout = run.outputs[\"handout\"]\n",
      "KeyError: 'handout'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 26, in faithfulness\n",
      "    return grade_faithfulness(rag_pipeline_run, None)\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 44, in grade_faithfulness\n",
      "    handout = example.outputs[\"handout\"]\n",
      "AttributeError: 'NoneType' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator document_relevance> on run a924e9e1-3178-40ef-896b-ed383be6cb9a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59230, Requested 1240. Please try again in 469ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 14, in document_relevance\n",
      "    scores.append(grade_doc(query, doc))\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/3432128128.py\", line 33, in grade_doc\n",
      "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59230, Requested 1240. Please try again in 469ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator document_relevance> on run ad584d69-2b7e-4fbc-9922-6820c43baa35: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59317, Requested 1118. Please try again in 435ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 14, in document_relevance\n",
      "    scores.append(grade_doc(query, doc))\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/3432128128.py\", line 33, in grade_doc\n",
      "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59317, Requested 1118. Please try again in 435ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator faithfulness> on run 74c73f2a-249d-4d5b-b235-7923abbca97c: AttributeError(\"'NoneType' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 41, in grade_faithfulness\n",
      "    handout = run.outputs[\"handout\"]\n",
      "KeyError: 'handout'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 26, in faithfulness\n",
      "    return grade_faithfulness(rag_pipeline_run, None)\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 44, in grade_faithfulness\n",
      "    handout = example.outputs[\"handout\"]\n",
      "AttributeError: 'NoneType' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator document_relevance> on run b3520e3f-3c62-4fe7-8b47-29d3fac52c3d: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59737, Requested 1822. Please try again in 1.559s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 14, in document_relevance\n",
      "    scores.append(grade_doc(query, doc))\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/3432128128.py\", line 33, in grade_doc\n",
      "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59737, Requested 1822. Please try again in 1.559s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator faithfulness> on run b3520e3f-3c62-4fe7-8b47-29d3fac52c3d: AttributeError(\"'NoneType' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 41, in grade_faithfulness\n",
      "    handout = run.outputs[\"handout\"]\n",
      "KeyError: 'handout'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 26, in faithfulness\n",
      "    return grade_faithfulness(rag_pipeline_run, None)\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 44, in grade_faithfulness\n",
      "    handout = example.outputs[\"handout\"]\n",
      "AttributeError: 'NoneType' object has no attribute 'outputs'\n",
      "Error running evaluator <DynamicRunEvaluator faithfulness> on run a924e9e1-3178-40ef-896b-ed383be6cb9a: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 58893, Requested 1973. Please try again in 866ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 26, in faithfulness\n",
      "    return grade_faithfulness(rag_pipeline_run, None)\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 47, in grade_faithfulness\n",
      "    result = grader_faithfulness.invoke({\"handout\": handout, \"contexts\": contexts}).list_of_false\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 58893, Requested 1973. Please try again in 866ms. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator faithfulness> on run ad584d69-2b7e-4fbc-9922-6820c43baa35: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59763, Requested 2699. Please try again in 2.462s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 26, in faithfulness\n",
      "    return grade_faithfulness(rag_pipeline_run, None)\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 47, in grade_faithfulness\n",
      "    result = grader_faithfulness.invoke({\"handout\": handout, \"contexts\": contexts}).list_of_false\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59763, Requested 2699. Please try again in 2.462s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator faithfulness> on run 60616e5b-0067-42d9-a2ef-3a8d99ac5eba: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59156, Requested 3851. Please try again in 3.007s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 26, in faithfulness\n",
      "    return grade_faithfulness(rag_pipeline_run, None)\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 47, in grade_faithfulness\n",
      "    result = grader_faithfulness.invoke({\"handout\": handout, \"contexts\": contexts}).list_of_false\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 2393, in invoke\n",
      "    input = step.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/runnables/base.py\", line 4427, in invoke\n",
      "    return self.bound.invoke(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 170, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 599, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 456, in generate\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 446, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_core/language_models/chat_models.py\", line 671, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langchain_openai/chat_models/base.py\", line 522, in _generate\n",
      "    response = self.client.create(messages=message_dicts, **params)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1005, in _request\n",
      "    return self._retry_request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1053, in _retry_request\n",
      "    return self._request(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-3.5-turbo-0125 in organization org-hIcQ6WHRm8UbxJ53MIHwRhVG on tokens per min (TPM): Limit 60000, Used 59156, Requested 3851. Please try again in 3.007s. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Error running evaluator <DynamicRunEvaluator faithfulness> on run 4b63bd9e-c7eb-4a60-af83-19b8615c2e30: AttributeError(\"'NoneType' object has no attribute 'outputs'\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 41, in grade_faithfulness\n",
      "    handout = run.outputs[\"handout\"]\n",
      "KeyError: 'handout'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/2776572676.py\", line 26, in faithfulness\n",
      "    return grade_faithfulness(rag_pipeline_run, None)\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_53709/371880261.py\", line 44, in grade_faithfulness\n",
      "    handout = example.outputs[\"handout\"]\n",
      "AttributeError: 'NoneType' object has no attribute 'outputs'\n"
     ]
    }
   ],
   "source": [
    "eval_llm_allcases_2(rag_bot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc3492-d5e1-426e-9428-01d2083cd48e",
   "metadata": {},
   "source": [
    "## LLM grading based on custom metrics\n",
    "- jargon\n",
    "- reference list\n",
    "- template format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a519e-40f6-4cfd-b9f9-27cf48252fa7",
   "metadata": {},
   "source": [
    "## Human feedback of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593e441-f1d7-4521-96a2-ca060923a33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
