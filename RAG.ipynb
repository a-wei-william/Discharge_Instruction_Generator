{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7d071a-daf7-40dc-a652-860a71a06fd8",
   "metadata": {},
   "source": [
    "# Task List\n",
    "- GPT3.5 LLM evaluator for grading context relevance is inaccurate\n",
    "- for eval on all cases, a way to group both experiments in one place; https://docs.smith.langchain.com/how_to_guides/evaluation/evaluate_on_intermediate_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6c287f-3b06-4b73-8b54-87a893580a60",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaac6de6-2ef6-4586-9184-8d93a3990363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "\n",
    "from _global import path_to_resources, hf_embed\n",
    "import templates\n",
    "\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import langsmith\n",
    "from langsmith import traceable, trace\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca634257-130b-4d4b-8f7f-0e8425dd5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up retriever\n",
    "db = Chroma(collection_name=\"main_collection\", persist_directory=f\"{path_to_resources}/db_wiki\", embedding_function=hf_embed)\n",
    "retriever = db.as_retriever(\n",
    "                search_type = \"similarity\",\n",
    "                search_kwargs = {\"k\":4},\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6392a02-2836-4da9-b7c1-8c4193b6952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langsmith setup\n",
    "project_name = \"ED-handout\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732bba5-125c-43c1-94b3-5f35c6158a0f",
   "metadata": {},
   "source": [
    "# RAG Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b228d657-be08-4d01-94af-fa08b78d3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagBot:\n",
    "    def __init__(self, retriever, templates, model: str = \"gpt-3.5-turbo-1106\"):\n",
    "        self._retriever = retriever\n",
    "        self._llm_gpt = ChatOpenAI(model_name=model, temperature=0)\n",
    "        self._llm_llama = Ollama(model=\"llama2:13b\", temperature=0)\n",
    "        self.templates = templates\n",
    "        self.queries = {\n",
    "            \"definition\": \"definition of {diagnosis}\",\n",
    "            \"presentation\": \"manifestations of {diagnosis}\",\n",
    "            \"course\": \"natural history of {diagnosis}\",\n",
    "            \"management\": \"treatment and management for {diagnosis}\",\n",
    "            \"follow_up\": \"follow-up plan for {diagnosis}\",\n",
    "            \"redflags\": \"signs and symptoms that indicate the need for urgent medical attention for patients with {diagnosis}\",\n",
    "        }\n",
    "\n",
    "    \n",
    "    @traceable\n",
    "    def diagnosis_extraction(self, assessment):\n",
    "        \"\"\"Extracts diagnosis from physician's assessment of the patient\"\"\"\n",
    "        prompt_extract_diagnosis = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",self.templates.extract_diagnosis_system),\n",
    "            (\"human\", \"{assessment}\")\n",
    "        ])\n",
    "        chain_diagnosis = prompt_extract_diagnosis | self._llm_gpt\n",
    "        \n",
    "        return chain_diagnosis.invoke({\"assessment\":assessment}).content\n",
    "\n",
    "    \n",
    "    def make_queries(self, diagnosis):\n",
    "        \"\"\"Uses the diagnosis to populate dict of queries that will be used to retreive context from db\"\"\"\n",
    "        return {key: value.format(diagnosis=diagnosis) for key, value in self.queries.items()}\n",
    "\n",
    "    \n",
    "    @traceable(run_type=\"retriever\")\n",
    "    def _retrieve_docs(self, query):\n",
    "        return self._retriever.invoke(query)\n",
    "\n",
    "    \n",
    "    def get_contexts(self, queries):\n",
    "        \"\"\"returns dict with tuples of (query, contexts)\"\"\"\n",
    "        contexts = {}\n",
    "        for k, query in queries.items():\n",
    "            contexts[k] = (query, self._retrieve_docs(query))\n",
    "        \n",
    "        return contexts\n",
    "\n",
    "\n",
    "    def compress_contexts(self, q_c):\n",
    "        prompt_compress = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.templates.compress_context_system),\n",
    "            (\"human\", self.templates.compress_context_human)\n",
    "        ])\n",
    "        chain_compress = prompt_compress | self._llm_gpt\n",
    "\n",
    "        return chain_compress.invoke({\"query\": q_c[0], \"context\": q_c[1]}).content\n",
    "\n",
    "    \n",
    "    @traceable()\n",
    "    def retrieval_steps(self, assessment, eval=False):\n",
    "        \"\"\"All the steps to prep the contexts for final handout generation\"\"\"    \n",
    "        diagnosis = self.diagnosis_extraction(assessment)\n",
    "        queries = self.make_queries(diagnosis)\n",
    "        contexts = self.get_contexts(queries)\n",
    "\n",
    "        return {\"contexts\": contexts, \"diagnosis\": diagnosis}\n",
    "        \n",
    "    \n",
    "    @traceable()\n",
    "    def make_handout(self, assessment, md_plan):\n",
    "        _run_input = self.retrieval_steps(assessment)\n",
    "        _contexts = _run_input[\"contexts\"]\n",
    "        diagnosis = _run_input[\"diagnosis\"]\n",
    "\n",
    "        return self._make_handout(_contexts, diagnosis, md_plan)\n",
    "        \n",
    "    @traceable()\n",
    "    def _make_handout(self, _contexts, diagnosis, md_plan):\n",
    "        \"\"\"separate this part from the complete make_handout chain so can be used in langsmith for testing\"\"\"\n",
    "        # compression\n",
    "        contexts = {}\n",
    "        for k, q_c in _contexts.items():\n",
    "            contexts[k] = self.compress_contexts(q_c)\n",
    "\n",
    "        # make handout\n",
    "        prompt_make_handout = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",self.templates.handout_generation_system),\n",
    "            (\"human\", self.templates.handout_generation_human),\n",
    "        ])\n",
    "        chain_make_handout = prompt_make_handout | self._llm_gpt\n",
    "        response = chain_make_handout.invoke({\n",
    "            \"context_definition\": contexts[\"definition\"],\n",
    "            \"context_presentation\": contexts[\"presentation\"],\n",
    "            \"context_course\": contexts[\"course\"],\n",
    "            \"context_management\": contexts[\"management\"],\n",
    "            \"context_follow_up\": contexts[\"follow_up\"],\n",
    "            \"context_redflags\": contexts[\"redflags\"],\n",
    "            \"context_md_plan\": md_plan,\n",
    "        })\n",
    "        \n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        return {\n",
    "            \"diagnosis\": diagnosis,\n",
    "            \"contexts\": \"\\n\".join(contexts.values()) + \"\\n\" + md_plan,\n",
    "            \"handout\": response.content\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a0f810ed-9511-4576-b64d-67b34f25a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_bot = RagBot(retriever, templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffc37dd4-127c-4c5f-96db-89c1e8b85b83",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# test that extraction works and works with langsmith\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiagnosis extraction\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchain\u001b[39m\u001b[38;5;124m\"\u001b[39m, project_name\u001b[38;5;241m=\u001b[39mproject_name, inputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massessment\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5yo M with viral-triggered asthma\u001b[39m\u001b[38;5;124m\"\u001b[39m}) \u001b[38;5;28;01mas\u001b[39;00m rt:\n\u001b[0;32m----> 4\u001b[0m     output \u001b[38;5;241m=\u001b[39m rag_bot\u001b[38;5;241m.\u001b[39mdiagnosis_extraction(\u001b[43minputs\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massessment\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m     rt\u001b[38;5;241m.\u001b[39mend(outputs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m: output})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inputs' is not defined"
     ]
    }
   ],
   "source": [
    "# test that extraction works and works with langsmith\n",
    "\n",
    "with trace(\"Diagnosis extraction\", \"chain\", project_name=project_name, inputs={\"assessment\": \"5yo M with viral-triggered asthma\"}) as rt:\n",
    "    output = rag_bot.diagnosis_extraction(inputs[\"assessment\"])\n",
    "    rt.end(outputs={\"output\": output})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55ef3f-50cd-4157-8f41-dac745b4cbde",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fd4e5-11e3-437c-8c28-21ce3e3c2465",
   "metadata": {},
   "source": [
    "ways to use evaluations\n",
    "- to evaluate one part of the RAG pipeline\n",
    "    1. this runs part of the rag pipeline -> create a dataset -> evaluate based on the data; used when optimizing each section (e.g prompt engineering, experimenting with retrieval strategies)\n",
    "    2. run on one example (dataset with assessment and plan as input for one diagnosis); used to test debug for individual case\n",
    "    3. run for all common diagnoses dataset (Pt_cases); evaluate the RAG chain on a database\n",
    "- evaluate the whole pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481bf51-268e-4417-adf2-c1ae7bc42c06",
   "metadata": {},
   "source": [
    "## Doc grader\n",
    "- given a diagnosis, create dataset of query + doc for each doc retrieved from each query\n",
    "- run experiement on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6887816a-7680-4cef-b0e5-a9fe19d2d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"0-2 score based on relevance of doc.\n",
    "    * 0: irrelevant diagnosis \\n\n",
    "    * 1: correct diagnosis, but does not contain information to anser the user question \\n\n",
    "    * 2: correct diagnosis and contains information to answer the user question). \\n    \n",
    "    \"\"\"\n",
    "\n",
    "    score: str = Field(description=\"Documents grade based on correct diagnosis and relevant information\")\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"\n",
    "    You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    The content of the document can be found in page_content. Give a score for the document using the scoring system below. \n",
    "    Scoring: \n",
    "    * 0: irrelevant diagnosis \\n\n",
    "    * 1: correct diagnosis, but does not contain information to anser the user question \\n\n",
    "    * 2: correct diagnosis and contains information to answer the user question). \\n\n",
    "    \n",
    "    \n",
    "\"\"\"\n",
    "prompt_gradedoc = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"RETRIEVED DOCUMENT: \\n\\n {document} \\n\\n USER QUESTION: {query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt_gradedoc | structured_llm_grader\n",
    "\n",
    "def grade_doc(query, doc) -> dict:\n",
    "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
    "    return {\"key\": \"grade\", \"score\": int(grade.score), \"comment\": \"grade for doc\"}\n",
    "\n",
    "def grade_docs(run, example) -> dict:\n",
    "    grade = retrieval_grader.invoke({\"query\": example.inputs[\"query\"], \"document\": example.inputs[\"context\"]})\n",
    "    return {\"key\": \"grade\", \"score\": int(grade.score), \"comment\": \"grade for doc\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "479b7eb9-786a-4d54-b30f-b624f7cf5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_relevance(diagnosis, context_dict, dataset_name):\n",
    "    \"\"\"Takes query_context dictionary and create a dataset for {diagnosis} to evaluate the relevance of retrieved context\"\"\"\n",
    "    client = langsmith.Client()\n",
    "    \n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=f\"Test context relevance for docs retreiived for {diagnosis}\",\n",
    "    )\n",
    "\n",
    "    for query, q_c in context_dict.values(): #each document should be an example in the dataset        \n",
    "        for doc in q_c:\n",
    "            client.create_examples(\n",
    "                inputs=[{\"query\": query, \"context\": doc}],\n",
    "                dataset_id=dataset.id,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f2d24128-e746-48bd-8ec9-e3e730a23097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval type: 1\n",
    "\n",
    "def eval_context_relevance(rag_bot, assessment):\n",
    "    retrieved = rag_bot.retrieval_steps(assessment) # dict of query:context\n",
    "    context_dict = retrieved[\"contexts\"]\n",
    "    diagnosis = retrieved[\"diagnosis\"]\n",
    "\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d, %H:%M:%S')\n",
    "    dataset_name = f\"Queries_Docs_{diagnosis}_{current_time}\"\n",
    "    create_dataset_relevance(diagnosis, context_dict, dataset_name)\n",
    "        \n",
    "    evaluate(\n",
    "        lambda x:x,\n",
    "        data=dataset_name,\n",
    "        evaluators=[grade_docs],\n",
    "        experiment_prefix=\"Context-relevance-\",\n",
    "        metadata={\n",
    "            \"model\": \"oai\",\n",
    "            \"diagnosis\":diagnosis\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c33348ea-1008-4e52-a7fb-6d4ff9882bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Context-relevance--feef56a0' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/2c62c230-5ffb-4be1-a160-67b532c3d537/compare?selectedSessions=2d4ce06b-d884-4f5f-bca6-6dc74cff05da\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc66df6bbe8a4bf8b2887ec1dfe70cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_context_relevance(rag_bot, \"5yo M, viral triggered asthma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffa5be-acaa-4a9e-bb05-456b47c18b9b",
   "metadata": {},
   "source": [
    "## Ground truth checker\n",
    "** LLM gives different evaluation each time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b4ec3cb-0292-4f9b-98a7-61f7713650e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in use; used once to create dataset for eval_groundtruth_evaluator_prompt\n",
    "def create_dataset_groundtruth(handout, context, dataset_name):\n",
    "    \"\"\"Takes handout and contexts used to make a dataset\"\"\"\n",
    "    client = langsmith.Client()\n",
    "    \n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=f\"Test whether handout for {diagnosis} is based on provided context\",\n",
    "    )\n",
    "\n",
    "    # **Preprocess context to remove headings\n",
    "    client.create_examples(\n",
    "        inputs=[{\"contexts\": contexts}],\n",
    "        outputs=[{\"handout\": handout}],\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2a8f8bc-5f7e-49a6-a636-598a2694ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroundTruth(BaseModel):\n",
    "    \"\"\"List facts in handout not based on ground truth\"\"\"\n",
    "    list_of_false: List[str] = Field(description=\"List of facts in handout not based on ground truth\")\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GroundTruth)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"\n",
    "You are an expert assessor tasked with evaluating whether the generated text (provided by the user between the XML tags GENERATED TEXT) is factually based on the provided context (provided by the user between the XML tags CONTEXT).  \n",
    "\n",
    "Follow these steps:\n",
    "    Step 1: Read the provided context carefully. Understand the information presented in the context.\n",
    "    Step 2: Analyze each sentence in the GENERATED TEXT. Compare it with the provided CONTEXT to determine its factual basis. Sentences in the GENERATED TEXT that are similar (but not verbatim) to the sentences in provided CONTEXT, but are still factually aligned are considered factually based on the CONTEXT.\n",
    "    Step 3: Identify sentences in the GENERATED TEXT that are not factually based on the context (not factually supported by the provided CONTEXT or directly contradicts the provided CONTEXT.) \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "<GENERATED TEXT>\n",
    "{handout} \n",
    "</GENERATED TEXT>\n",
    "\n",
    "<CONTEXT>\n",
    "{contexts}\n",
    "</CONTEXT>\n",
    "\"\"\"\n",
    "\n",
    "prompt_groundtruth = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "grader_groundtruth = prompt_groundtruth | structured_llm_grader\n",
    "\n",
    "def grade_groundtruth(run, example):\n",
    "    try: # try to get outputs from run, otherwise it is from dataset\n",
    "        handout = run.outputs[\"handout\"]\n",
    "        contexts = run.outputs[\"contexts\"]\n",
    "    except KeyError:\n",
    "        handout = example.outputs[\"handout\"]\n",
    "        contexts = example.inputs[\"contexts\"]\n",
    "        \n",
    "    result = grader_groundtruth.invoke({\"handout\": handout, \"contexts\": contexts}).list_of_false\n",
    "    count = len(result)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"count\", \"score\": count, \"comment\": \"num infactual sentences\",\n",
    "        \"sentences\": result\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cc83451-f127-4d4f-aca1-f4145f92f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval type: 1; to experiment with prompt engineering for LLM evaluator using dataset on context and handout\n",
    "\n",
    "def eval_groundtruth_evaluator_prompt(rag_bot):\n",
    "    \"\"\"to evaluate prompt for LLM assessor for ground truth evaluation task\"\"\"\n",
    "    dataset_name = \"Context_groundtruth\"\n",
    "        \n",
    "    evaluate(\n",
    "        lambda x:x,\n",
    "        data=dataset_name,\n",
    "        evaluators=[grade_groundtruth],\n",
    "        experiment_prefix=\"Groundtruth-prompt-\",\n",
    "        metadata={\n",
    "            \"model\": \"oai\",\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b130477-99a2-4507-8300-0201207474d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Groundtruth-prompt--21269dce' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/8c0209e9-1fa7-4eae-9ee6-657491c07e75/compare?selectedSessions=bbef5376-38e7-4ae5-9b97-889c19d0c4b9\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438360a9a6a94f91be4c21c95602e777",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_groundtruth_evaluator_prompt(\n",
    "    rag_bot,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a281f1ae-a83f-4f24-86f1-7c3c57a3e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval type: 2; to evaluate context ground truth for one diagnosis by running the whole rag chain. No dataset created in this process\n",
    "def eval_context_groundtruth(rag_bot, assessment, md_plan):\n",
    "    evaluate(\n",
    "        lambda x: rag_bot.make_handout(assessment, md_plan),\n",
    "        data=\"Ground_truth\", # dummy dataset\n",
    "        evaluators=[grade_groundtruth],\n",
    "        experiment_prefix=\"Groundtruth-\",\n",
    "        metadata={\n",
    "            \"model\": \"oai\",\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "591ea43b-4f87-4d96-b002-6ea2fd7ea73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Groundtruth--52310ebe' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/6ce351fc-f177-4f74-9322-8c8cd25579d4/compare?selectedSessions=cfa71343-37ed-4085-a1f9-7651080d4e0d\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88369cb28892463580813783ec61981e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43meval_context_groundtruth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrag_bot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43massessment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m5yo M, first asthma exacerbation, virally triggered\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmd_plan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m-continue ventolin q4h \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m -continue flovent 125mcg qdaily \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m -follow-up with your family doctor in 2 days\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 4\u001b[0m, in \u001b[0;36meval_context_groundtruth\u001b[0;34m(rag_bot, assessment, md_plan)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_context_groundtruth\u001b[39m(rag_bot, assessment, md_plan):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrag_bot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_handout\u001b[49m\u001b[43m(\u001b[49m\u001b[43massessment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd_plan\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGround_truth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# doesnt actu\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mgrade_groundtruth\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGroundtruth-\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py:235\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, client, blocking)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(\n\u001b[1;32m     76\u001b[0m     target: TARGET_T,\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;241m/\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     86\u001b[0m     blocking: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ExperimentResults:\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate a target system or function on a given dataset.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;124;03m        View the evaluation results for experiment:...\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43msummary_evaluators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msummary_evaluators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexperiment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocking\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py:814\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(target, data, evaluators, summary_evaluators, metadata, experiment_prefix, description, max_concurrency, client, blocking, experiment)\u001b[0m\n\u001b[1;32m    811\u001b[0m results \u001b[38;5;241m=\u001b[39m ExperimentResults(manager)\n\u001b[1;32m    812\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blocking:\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;66;03m# Wait for the evaluation to complete.\u001b[39;00m\n\u001b[0;32m--> 814\u001b[0m     \u001b[43mresults\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py:410\u001b[0m, in \u001b[0;36mExperimentResults.wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wait for the evaluation runner to complete.\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \n\u001b[1;32m    407\u001b[0m \u001b[38;5;124;03m    This method blocks the current thread until the evaluation runner has\u001b[39;00m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;124;03m    finished its execution.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/llm/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator grade_groundtruth> on run fa5b4bb6-3c62-4ad0-a5a2-4acf92e1a0ac: KeyError('handout')\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 279, in evaluate_run\n",
      "    result = self.func(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 565, in wrapper\n",
      "    raise e\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/run_helpers.py\", line 560, in wrapper\n",
      "    function_result = run_container[\"context\"].run(\n",
      "  File \"/var/folders/rc/1l8xrxl962j_sth8dsw54l6h0000gn/T/ipykernel_11038/3166412940.py\", line 40, in grade_groundtruth\n",
      "    result = grader_groundtruth.invoke({\"handout\": example.outputs[\"handout\"], \"contexts\": example.inputs[\"contexts\"]}).list_of_false\n",
      "KeyError: 'handout'\n",
      "Exception in thread Exception in threading.excepthook:\n",
      "Exception ignored in thread started by: <bound method Thread._bootstrap of <Thread(Thread-100 (<lambda>), stopped 11733692416)>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/threading.py\", line 973, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/threading.py\", line 1018, in _bootstrap_inner\n",
      "    self._invoke_excepthook(self)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/threading.py\", line 1336, in invoke_excepthook\n",
      "    local_print(\"Exception in threading.excepthook:\",\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/ipykernel/iostream.py\", line 573, in flush\n",
      "    self.pub_thread.schedule(self._flush)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/ipykernel/iostream.py\", line 266, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 696, in send\n",
      "      File \"zmq/backend/cython/socket.pyx\", line 742, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 783, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 138, in zmq.backend.cython.socket._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n",
      "Exception ignored in sys.unraisablehook: <built-in function unraisablehook>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/ipykernel/iostream.py\", line 573, in flush\n",
      "      File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/ipykernel/iostream.py\", line 266, in schedule\n",
      "    self._event_pipe.send(b\"\")\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/zmq/sugar/socket.py\", line 696, in send\n",
      "    return super().send(data, flags=flags, copy=copy, track=track)\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 742, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 783, in zmq.backend.cython.socket.Socket.send\n",
      "  File \"zmq/backend/cython/socket.pyx\", line 138, in zmq.backend.cython.socket._check_closed\n",
      "zmq.error.ZMQError: Socket operation on non-socket\n"
     ]
    }
   ],
   "source": [
    "eval_context_groundtruth(\n",
    "    rag_bot,\n",
    "    assessment=\"5yo M, first asthma exacerbation, virally triggered\", \n",
    "    md_plan=\"-continue ventolin q4h \\n -continue flovent 125mcg qdaily \\n -follow-up with your family doctor in 2 days\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f83d873d-af74-49f6-adb3-cd517015feb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Groundtruth--091700f9' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/8c0209e9-1fa7-4eae-9ee6-657491c07e75/compare?selectedSessions=71f30369-3ab9-4fab-97ad-34e61d6e4790\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d9022184ba457fbe50ed686f8133ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_groundtruth(rag_bot, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "140409c5-5cc0-4655-8b61-8b83ad546e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **not used** eval type: 3; to run whole rag chain on all diagnoses in Pt_cases and generate dataset for possible eval\n",
    "import time\n",
    "def eval_llm_allcases(rag_bot):\n",
    "    client = langsmith.Client()\n",
    "    cases = client.list_examples(dataset_id=\"e3957f7c-e232-4541-beef-d7216ab12241\")\n",
    "\n",
    "    i = 0\n",
    "    for case in cases:\n",
    "        # eval for grading the document\n",
    "        assessment = case.inputs[\"assessment\"]\n",
    "        md_plan = case.inputs[\"plan\"]\n",
    "        \n",
    "        retrieved = rag_bot.retrieval_steps(assessment) # dict of query:context\n",
    "        context_dict = retrieved[\"contexts\"]\n",
    "        diagnosis = retrieved[\"diagnosis\"]\n",
    "    \n",
    "        current_time = datetime.now().strftime('%Y-%m-%d, %H:%M:%S')\n",
    "        dataset_name = f\"LLM_{diagnosis}_{current_time}\"\n",
    "        create_dataset_relevance(diagnosis, context_dict, dataset_name)\n",
    "\n",
    "        time.sleep(90)\n",
    "        evaluate(\n",
    "            lambda x:x,\n",
    "            data=dataset_name,\n",
    "            evaluators=[grade_docs],\n",
    "            experiment_prefix=\"LLM-context-relevance-\",\n",
    "            metadata={\n",
    "                \"model\": \"oai\",\n",
    "                \"eval\": \"doc_grade\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "        time.sleep(90)\n",
    "        # eval for ground truth\n",
    "        evaluate(\n",
    "            lambda x: rag_bot._make_handout(context_dict, diagnosis, md_plan),\n",
    "            data=\"Ground_truth\",\n",
    "            evaluators=[grade_groundtruth],\n",
    "            experiment_prefix=\"LLM-groundtruth\",\n",
    "            metadata={\n",
    "                \"model\": \"oai\",\n",
    "                \"eval\": \"groundtruth\",\n",
    "            },\n",
    "        )\n",
    "        i+=1\n",
    "        if i ==2:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2a90a08d-338a-421e-ab7a-c733dd222c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_relevance(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A very simple evaluator that checks to see if the input of the retrieval step exists\n",
    "    in the retrieved docs.\n",
    "    \"\"\"\n",
    "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"make_handout\")\n",
    "    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieval_steps\")\n",
    "\n",
    "    context_dict = retrieve_run.outputs[\"contexts\"]\n",
    "\n",
    "    scores = []\n",
    "    for query, q_c in context_dict.values(): #each document should be an example in the dataset        \n",
    "        for doc in q_c:\n",
    "            scores.append(grade_doc(query, doc))\n",
    "            \n",
    "    return {\"results\": scores}\n",
    "    \n",
    "\n",
    "def ground_truth(root_run: Run, example: Example) -> dict:\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks to see the answer is grounded in the documents\n",
    "    \"\"\"\n",
    "    # Get documents and answer\n",
    "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"make_handout\")\n",
    "\n",
    "    score = grade_groundtruth(rag_pipeline_run, None)\n",
    "    return {\"key\": \"num_false_facts\", \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "617e5b10-8f60-4c24-90f7-222fc7ed6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_llm_allcases_1(rag_bot):\n",
    "    experiment_results = evaluate(\n",
    "        lambda inputs: rag_bot.make_handout(assessment=inputs[\"assessment\"], md_plan=inputs[\"plan\"]),\n",
    "        data=\"Dummy_pt_case\",\n",
    "        evaluators=[document_relevance, ground_truth],\n",
    "        experiment_prefix=datetime.now().strftime('%Y-%m-%d, %H:%M:%S')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ee06f395-6eaf-4cad-87fd-cd5117941b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: '2024-06-02, 18:33:25-0caa1194' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/281774a1-94f9-47f1-9846-88086ceec7d5/compare?selectedSessions=dd801a42-7c33-45f6-a899-e091ef89b6d7\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17df88d629c48f6a8b8729490783a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error running evaluator <DynamicRunEvaluator ground_truth> on run 774a37b1-5990-4eb2-a3d6-61d9b5e6d99e: ValueError(\"Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score'; got {'key': 'num_false_facts', 'score': {'key': 'count', 'score': 7, 'comment': 'num infactual sentences', 'sentences': ['- Administer the prescribed dexamethasone at 3pm tomorrow as directed by the doctor.', '- Use over-the-counter medications for pain and fever to keep your child comfortable.', '- Keep your child calm and comfortable, and consider using cool or warm mist, although its effectiveness is not clear.', '- Seek immediate medical attention if your child experiences:', '- Inspiratory stridor (a high-pitched sound when breathing in)', '- Blue or bluish-colored lips', '- Decrease in the level of alertness']}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 204, in _coerce_evaluation_result\n",
      "    return EvaluationResult(**{\"source_run_id\": source_run_id, **result})\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/pydantic/v1/main.py\", line 341, in __init__\n",
      "    raise validation_error\n",
      "pydantic.v1.error_wrappers.ValidationError: 3 validation errors for EvaluationResult\n",
      "score\n",
      "  value is not a valid boolean (type=value_error.strictbool)\n",
      "score\n",
      "  value is not a valid integer (type=type_error.integer)\n",
      "score\n",
      "  value is not a valid float (type=type_error.float)\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/_runner.py\", line 1216, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 284, in evaluate_run\n",
      "    return self._format_result(result, source_run_id)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 241, in _format_result\n",
      "    return self._coerce_evaluation_results(result, source_run_id)\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 224, in _coerce_evaluation_results\n",
      "    return self._coerce_evaluation_result(\n",
      "  File \"/Users/a_wei/miniconda3/envs/llm/lib/python3.10/site-packages/langsmith/evaluation/evaluator.py\", line 206, in _coerce_evaluation_result\n",
      "    raise ValueError(\n",
      "ValueError: Expected an EvaluationResult object, or dict with a metric 'key' and optional 'score'; got {'key': 'num_false_facts', 'score': {'key': 'count', 'score': 7, 'comment': 'num infactual sentences', 'sentences': ['- Administer the prescribed dexamethasone at 3pm tomorrow as directed by the doctor.', '- Use over-the-counter medications for pain and fever to keep your child comfortable.', '- Keep your child calm and comfortable, and consider using cool or warm mist, although its effectiveness is not clear.', '- Seek immediate medical attention if your child experiences:', '- Inspiratory stridor (a high-pitched sound when breathing in)', '- Blue or bluish-colored lips', '- Decrease in the level of alertness']}}\n"
     ]
    }
   ],
   "source": [
    "eval_llm_allcases_1(rag_bot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc3492-d5e1-426e-9428-01d2083cd48e",
   "metadata": {},
   "source": [
    "## LLM grading based on custom metrics\n",
    "- jargon\n",
    "- reference list\n",
    "- template format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a519e-40f6-4cfd-b9f9-27cf48252fa7",
   "metadata": {},
   "source": [
    "## Human feedback of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593e441-f1d7-4521-96a2-ca060923a33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
