{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d7d071a-daf7-40dc-a652-860a71a06fd8",
   "metadata": {},
   "source": [
    "# Task List\n",
    "- structured intput and output for context compression\n",
    "- GPT3.5 LLM evaluator for grading context relevance is inaccurate\n",
    "- was cleaning up code and adding comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6c287f-3b06-4b73-8b54-87a893580a60",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaac6de6-2ef6-4586-9184-8d93a3990363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "from operator import itemgetter\n",
    "from typing import Optional\n",
    "from _global import path_to_resources, hf_embed\n",
    "import templates\n",
    "\n",
    "from langchain.callbacks.tracers import LangChainTracer\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "import langsmith\n",
    "from langsmith import traceable, trace\n",
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca634257-130b-4d4b-8f7f-0e8425dd5d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up retriever\n",
    "db = Chroma(collection_name=\"main_collection\", persist_directory=f\"{path_to_resources}/db_main\", embedding_function=hf_embed)\n",
    "retriever = db.as_retriever(\n",
    "                search_type = \"similarity\",\n",
    "                search_kwargs = {\"k\":4},\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6392a02-2836-4da9-b7c1-8c4193b6952f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# langsmith setup\n",
    "project_name = \"ED-handout\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732bba5-125c-43c1-94b3-5f35c6158a0f",
   "metadata": {},
   "source": [
    "# RAG Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "923bdf8f-201c-43dd-99ec-b23423c4fd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedDoc(BaseModel):\n",
    "    \"\"\"Represents a single compressed document containing relevant information extracted for a specific query.\"\"\"\n",
    "    id: int = Field(description=\"The unique identifier for the document, representing its position in the retrieval sequence.\")\n",
    "    context: str = Field(description=\"The relevant text extracted verbatim from the document. Special characters are properly escaped.\")\n",
    "    source: Optional[str] = Field(description=\"Source of the information\")\n",
    "\n",
    "    @validator('context')\n",
    "    def escape_special_characters(cls, value):\n",
    "        return json.dumps(value)[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df3e8183-f0f7-44ce-9522-97b256466816",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompressedDocs(BaseModel):\n",
    "    \"\"\"Represents a collection of compressed documents, all containing relevant information extracted for a specific query. Use by PydanticOutputParser tool.\"\"\"\n",
    "    contexts: List[CompressedDoc] = Field(description=\"A list of CompressedDoc objects, each representing a compressed document with extracted relevant information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b228d657-be08-4d01-94af-fa08b78d3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagBot:\n",
    "    \"\"\"Bot that handles different steps of RAG\"\"\"\n",
    "    def __init__(self, retriever, templates, model: str = \"gpt-3.5-turbo-1106\"):\n",
    "        self._retriever = retriever\n",
    "        self._llm_gpt = ChatOpenAI(model_name=model, temperature=0)\n",
    "        self._llm_compressor = self._llm_gpt.with_structured_output(CompressedDocs)\n",
    "        self._llm_llama = Ollama(model=\"llama2:13b\", temperature=0)\n",
    "        self.templates = templates\n",
    "        self._queries = { # old queries\n",
    "            \"definition\": \"definition of {diagnosis}\",\n",
    "            \"presentation\": \"manifestations of {diagnosis}\",\n",
    "            \"course\": \"natural history of {diagnosis}\",\n",
    "            \"management\": \"treatment and management for {diagnosis}\",\n",
    "            \"follow_up\": \"follow-up plan for {diagnosis}\",\n",
    "            \"redflags\": \"signs and symptoms that indicate the need to return to the emergency department for patients with {diagnosis}\",\n",
    "        }\n",
    "        self.queries = {\n",
    "            \"definition\": \"definition, description, and clinical criteria of {diagnosis}\",\n",
    "            \"presentation\": \"clinical presentation, signs, and symptoms of {diagnosis}\",\n",
    "            \"course\": \"natural history, progression, and stages of {diagnosis}\",\n",
    "            \"management\": \"treatment options, therapeutic interventions, and management strategies for {diagnosis}\",\n",
    "            \"follow_up\": \"follow-up plan and monitoring for {diagnosis}\",\n",
    "            \"redflags\": \"red flags, warning signs, and symptoms indicating need to return to emergency department for {diagnosis}\",\n",
    "        }\n",
    "\n",
    "\n",
    "    \n",
    "    @traceable\n",
    "    def diagnosis_extraction(self, assessment):\n",
    "        \"\"\"Extracts diagnosis from physician's assessment of the patient\"\"\"\n",
    "        prompt_extract_diagnosis = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",self.templates.extract_diagnosis_system),\n",
    "            (\"human\", \"{assessment}\")\n",
    "        ])\n",
    "        chain_diagnosis = prompt_extract_diagnosis | self._llm_gpt\n",
    "        \n",
    "        return chain_diagnosis.invoke({\"assessment\":assessment}).content\n",
    "\n",
    "    \n",
    "    def make_queries(self, diagnosis):\n",
    "        \"\"\"Uses the diagnosis to populate dict of queries that will be used to retreive context from db\"\"\"\n",
    "        return {key: value.format(diagnosis=diagnosis) for key, value in self.queries.items()}\n",
    "\n",
    "    \n",
    "    @traceable(run_type=\"retriever\")\n",
    "    def _retrieve_docs(self, query):\n",
    "        return self._retriever.invoke(query)\n",
    "\n",
    "    \n",
    "    def get_contexts(self, queries):\n",
    "        \"\"\"returns dict with tuples of (query, contexts)\"\"\"\n",
    "        contexts = {}\n",
    "        for k, query in queries.items():\n",
    "            contexts[k] = (query, self._retrieve_docs(query))\n",
    "        \n",
    "        return contexts\n",
    "\n",
    "\n",
    "    def compress_contexts(self, q_c):\n",
    "        \"\"\"contextual compression with llm\"\"\"\n",
    "        prompt_compress = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", self.templates.compress_context_system),\n",
    "            (\"human\", self.templates.compress_context_human)\n",
    "        ])\n",
    "        chain_compress = prompt_compress | self._llm_compressor\n",
    "\n",
    "        return chain_compress.invoke({\"query\": q_c[0], \"context\": q_c[1]})\n",
    "\n",
    "    \n",
    "    @traceable()\n",
    "    def retrieval_steps(self, assessment):\n",
    "        \"\"\"all the steps to prep the contexts for final handout generation\"\"\"    \n",
    "        diagnosis = self.diagnosis_extraction(assessment)\n",
    "        queries = self.make_queries(diagnosis)\n",
    "        contexts = self.get_contexts(queries)\n",
    "\n",
    "        return {\"contexts\": contexts, \"diagnosis\": diagnosis}\n",
    "        \n",
    "    \n",
    "    @traceable()\n",
    "    def make_handout(self, assessment, md_plan):\n",
    "        _run_input = self.retrieval_steps(assessment)\n",
    "        _contexts = _run_input[\"contexts\"]\n",
    "        diagnosis = _run_input[\"diagnosis\"]\n",
    "\n",
    "        # compression\n",
    "        contexts = {}\n",
    "        for k, q_c in _contexts.items():\n",
    "            time.sleep(60)\n",
    "            contexts[k] = self.compress_contexts(q_c)\n",
    "        \n",
    "        # make handout\n",
    "        time.sleep(60)\n",
    "        prompt_make_handout = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",self.templates.handout_generation_system),\n",
    "            (\"human\", self.templates.handout_generation_human),\n",
    "        ])\n",
    "        chain_make_handout = prompt_make_handout | self._llm_gpt\n",
    "        response = chain_make_handout.invoke({\n",
    "            \"diagnosis\": diagnosis,\n",
    "            \"context_definition\": contexts[\"definition\"],\n",
    "            \"context_presentation\": contexts[\"presentation\"],\n",
    "            \"context_course\": contexts[\"course\"],\n",
    "            \"context_management\": contexts[\"management\"],\n",
    "            \"context_follow_up\": contexts[\"follow_up\"],\n",
    "            \"context_redflags\": contexts[\"redflags\"],\n",
    "            \"context_md_plan\": md_plan,\n",
    "        })\n",
    "        \n",
    "        # Evaluators will expect \"answer\" and \"contexts\"\n",
    "        contexts_in_string = []\n",
    "        for arr in contexts.values():\n",
    "            contexts_in_string.append(\"\\n\".join([doc.context for doc in arr.contexts]))\n",
    "        contexts_in_string = \"\\n\\n\".join(contexts_in_string) + \"\\n\" + md_plan\n",
    "        \n",
    "        return {\n",
    "            \"diagnosis\": diagnosis,\n",
    "            \"contexts\": contexts_in_string,\n",
    "            \"handout\": response.content\n",
    "        }\n",
    "\n",
    "    @traceable()\n",
    "    def compression(self, assessment):\n",
    "        _run_input = self.retrieval_steps(assessment)\n",
    "        _contexts = _run_input[\"contexts\"]\n",
    "        diagnosis = _run_input[\"diagnosis\"]\n",
    "\n",
    "        # compression\n",
    "        contexts = {}\n",
    "        for k, q_c in _contexts.items():\n",
    "            contexts[k] = self.compress_contexts(q_c)\n",
    "\n",
    "        return contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0f810ed-9511-4576-b64d-67b34f25a759",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_bot = RagBot(retriever, templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c69f1239-4666-4f7c-a1a8-a634f77de00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith trace(\"Test compression output structure\", \"chain\", project_name=project_name, inputs=\"\") as rt:\\n    output = rag_bot.compression(\"7yo M with MSK injury - ankle sprain\")\\n    rt.end(outputs={\"output\": output})\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with trace(\"Test compression output structure\", \"chain\", project_name=project_name, inputs=\"\") as rt:\n",
    "    output = rag_bot.compression(\"7yo M with MSK injury - ankle sprain\")\n",
    "    rt.end(outputs={\"output\": output})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c55ef3f-50cd-4157-8f41-dac745b4cbde",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17fd4e5-11e3-437c-8c28-21ce3e3c2465",
   "metadata": {},
   "source": [
    "ways to use evaluations\n",
    "- to evaluate part of the RAG pipeline\n",
    "    1. **before handout generation**: this runs part of the rag pipeline -> create a dataset -> evaluate based on the data; used when optimizing each section (e.g prompt engineering, experimenting with retrieval strategies)\n",
    "    2. **after handout generation**: run on one example (dataset with assessment and plan as input for one diagnosis); used to test debug for individual case\n",
    "- evaluate the whole pipeline: run for all common diagnoses dataset (Pt_cases); evaluate the RAG chain on a database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0481bf51-268e-4417-adf2-c1ae7bc42c06",
   "metadata": {},
   "source": [
    "## Doc grader\n",
    "- given a diagnosis, create dataset of query + doc for each doc retrieved from each query\n",
    "- run experiement on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6887816a-7680-4cef-b0e5-a9fe19d2d267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\" Pydantic model used to format the output of the LLM when grading documents based on the diagnosis accuracy and relevance of information.\"\"\"\n",
    "    score: str = Field(description=\"Grade assigned to the document based on the accuracy of the diagnosis and relevance of the information provided. Possible values are 0, 1, and 2.\")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system = \"\"\"\n",
    "You are a grader assessing the relevance of a retrieved document to a user's question. \n",
    "The content of the document can be found in 'page_content'. \n",
    "\n",
    "Assign a score to the document using the scoring system below:\n",
    "\n",
    "# Scoring System\n",
    "- **0: Does not contain diagnosis** \n",
    "  - The document does not provide any information about the diagnosis in the user's question.\n",
    "\n",
    "- **1: Contains diagnosis, but does not contain information to answer the user's question** \n",
    "  - The document contains information about the diagnosis mentioned in the user's question, but it does not provide relevant information to answer the user's question.\n",
    "\n",
    "- **2: Contains both diagnosis and information to answer the user's question** \n",
    "  - The document contains information about the diagnosis mentioned in the user's question and provides relevant information to fully answer the user's question.\n",
    "\"\"\"\n",
    "\n",
    "prompt_gradedoc = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"RETRIEVED DOCUMENT: \\n\\n {document} \\n\\n USER QUESTION: {query}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = prompt_gradedoc | structured_llm_grader\n",
    "\n",
    "def grade_doc(query, doc) -> dict:\n",
    "    \"\"\"Grades one query and one corresponding document. used in eval type 3\"\"\"\n",
    "    grade = retrieval_grader.invoke({\"query\": query, \"document\": doc})\n",
    "    return {\"key\": \"grade\", \"score\": int(grade.score), \"comment\": \"grade for doc\"}\n",
    "\n",
    "def grade_docs(run, example) -> dict:\n",
    "    \"\"\"Grades all queries and corresponding documents in the db. used in eval type 1\"\"\"\n",
    "    grade = retrieval_grader.invoke({\"query\": example.inputs[\"query\"], \"document\": example.inputs[\"context\"]})\n",
    "    return {\"key\": \"grade\", \"score\": int(grade.score), \"comment\": \"grade for doc\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "479b7eb9-786a-4d54-b30f-b624f7cf5e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_relevance(diagnosis, context_dict, dataset_name):\n",
    "    \"\"\"Takes query_context dictionary and create a dataset for {diagnosis} to evaluate the relevance of retrieved context\"\"\"\n",
    "    client = langsmith.Client()\n",
    "    \n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=f\"Test context relevance for docs retreiived for {diagnosis}\",\n",
    "    )\n",
    "\n",
    "    for query, q_c in context_dict.values(): #each document should be an example in the dataset        \n",
    "        for doc in q_c:\n",
    "            client.create_examples(\n",
    "                inputs=[{\"query\": query, \"context\": doc}],\n",
    "                dataset_id=dataset.id,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d24128-e746-48bd-8ec9-e3e730a23097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval type: 1; used to asses the retrieved context's relevance\n",
    "\n",
    "def eval_context_relevance(rag_bot, assessment):\n",
    "    retrieved = rag_bot.retrieval_steps(assessment) # dict of query:context\n",
    "    context_dict = retrieved[\"contexts\"]\n",
    "    diagnosis = retrieved[\"diagnosis\"]\n",
    "\n",
    "    current_time = datetime.now().strftime('%Y-%m-%d, %H:%M:%S')\n",
    "    dataset_name = f\"Queries_Docs_{diagnosis}_{current_time}\"\n",
    "    create_dataset_relevance(diagnosis, context_dict, dataset_name)\n",
    "        \n",
    "    evaluate(\n",
    "        lambda x:x,\n",
    "        data=dataset_name,\n",
    "        evaluators=[grade_docs],\n",
    "        experiment_prefix=\"Context-relevance-\",\n",
    "        metadata={\n",
    "            \"model\": \"oai\",\n",
    "            \"diagnosis\":diagnosis\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c33348ea-1008-4e52-a7fb-6d4ff9882bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Context-relevance--08c46ad6' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/2d388e9e-aa66-4828-bb74-55674694c6ec/compare?selectedSessions=cf10fa9c-5e24-4ec1-a361-cee567d84e58\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f59374b218e4ef5b013e8aac78787dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_context_relevance(rag_bot, \"5yo M, viral triggered asthma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffa5be-acaa-4a9e-bb05-456b47c18b9b",
   "metadata": {},
   "source": [
    "## Faithful checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b4ec3cb-0292-4f9b-98a7-61f7713650e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not in use; used once to create dataset for eval_faithfulness_evaluator_prompt\n",
    "def create_dataset_faithfulness(handout, context, dataset_name):\n",
    "    \"\"\"Takes handout and contexts used to make a dataset\"\"\"\n",
    "    client = langsmith.Client()\n",
    "    \n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=f\"Test whether handout for {diagnosis} is based on provided context\",\n",
    "    )\n",
    "\n",
    "    # **Preprocess context to remove headings\n",
    "    client.create_examples(\n",
    "        inputs=[{\"contexts\": contexts}],\n",
    "        outputs=[{\"handout\": handout}],\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2a8f8bc-5f7e-49a6-a636-598a2694ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Faithfulness(BaseModel):\n",
    "    \"\"\"List facts in handout not based on ground truth\"\"\"\n",
    "    list_of_false: List[str] = Field(description=\"List of facts in handout not based on ground truth\")\n",
    "\n",
    "# LLM with function call \n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(Faithfulness)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"\n",
    "You are an expert assessor tasked with evaluating whether the generated text (provided by the user between the XML tags GENERATED TEXT) is factually based on the provided context (provided by the user between the XML tags CONTEXT).  \n",
    "\n",
    "Follow these steps:\n",
    "    Step 1: Read the provided context carefully. Understand the information presented in the context.\n",
    "    Step 2: Analyze each sentence in the GENERATED TEXT. Compare it with the provided CONTEXT to determine its factual basis. Sentences in the GENERATED TEXT that are similar (but not verbatim) to the sentences in provided CONTEXT, but are still factually aligned are considered factually based on the CONTEXT.\n",
    "    Step 3: Identify sentences in the GENERATED TEXT that are not factually based on the context (not factually supported by the provided CONTEXT or directly contradicts the provided CONTEXT.) \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "human = \"\"\"\n",
    "<GENERATED TEXT>\n",
    "{handout} \n",
    "</GENERATED TEXT>\n",
    "\n",
    "<CONTEXT>\n",
    "{contexts}\n",
    "</CONTEXT>\n",
    "\"\"\"\n",
    "\n",
    "prompt_faithfulness= ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human),\n",
    "    ]\n",
    ")\n",
    "\n",
    "grader_faithfulness= prompt_faithfulness| structured_llm_grader\n",
    "\n",
    "def grade_faithfulness(run, example):\n",
    "    try: # try to get outputs from run, otherwise it is from dataset\n",
    "        handout = run.outputs[\"handout\"]\n",
    "        contexts = run.outputs[\"contexts\"]\n",
    "    except KeyError:\n",
    "        handout = example.outputs[\"handout\"]\n",
    "        contexts = example.inputs[\"contexts\"]\n",
    "        \n",
    "    result = grader_faithfulness.invoke({\"handout\": handout, \"contexts\": contexts}).list_of_false\n",
    "    count = len(result)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"count\", \"score\": count, \"comment\": \"num infactual sentences\",\n",
    "        \"sentences\": result\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cc83451-f127-4d4f-aca1-f4145f92f63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval type: 1; to experiment with prompt engineering for LLM evaluator using dataset on context and handout\n",
    "\n",
    "def eval_faithfulness_evaluator_prompt(rag_bot):\n",
    "    \"\"\"to evaluate prompt for LLM assessor for faithfulness evaluation task\"\"\"\n",
    "    dataset_name = \"Context_faithfulness\"\n",
    "        \n",
    "    evaluate(\n",
    "        lambda x:x,\n",
    "        data=dataset_name,\n",
    "        evaluators=[grade_faithfulness],\n",
    "        experiment_prefix=\"Faithfulness-prompt-\",\n",
    "        metadata={\n",
    "            \"model\": \"oai\",\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b130477-99a2-4507-8300-0201207474d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_faithfulness_evaluator_prompt(rag_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a281f1ae-a83f-4f24-86f1-7c3c57a3e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval type: 2; to evaluate context faithfulness for one diagnosis by running the whole rag chain. No dataset created in this process\n",
    "def eval_context_faithfulness(rag_bot, assessment, md_plan):\n",
    "    evaluate(\n",
    "        lambda x: rag_bot.make_handout(assessment, md_plan),\n",
    "        data=\"Faithfulness\", # dummy dataset\n",
    "        evaluators=[grade_faithfulness],\n",
    "        experiment_prefix=\"Faithfulness-\",\n",
    "        metadata={\n",
    "            \"model\": \"oai\",\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "591ea43b-4f87-4d96-b002-6ea2fd7ea73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\neval_context_faithfulness(\\n    rag_bot,\\n    assessment=\"5yo M, first asthma exacerbation, virally triggered\", \\n    md_plan=\"-continue ventolin q4h \\n -continue flovent 125mcg qdaily \\n -follow-up with your family doctor in 2 days\",\\n)\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "eval_context_faithfulness(\n",
    "    rag_bot,\n",
    "    assessment=\"5yo M, first asthma exacerbation, virally triggered\", \n",
    "    md_plan=\"-continue ventolin q4h \\n -continue flovent 125mcg qdaily \\n -follow-up with your family doctor in 2 days\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935bf0de-f581-4c8b-92e0-ef0c53e562f5",
   "metadata": {},
   "source": [
    "## Full pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a90a08d-338a-421e-ab7a-c733dd222c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_relevance(root_run, example):\n",
    "    \"\"\"\n",
    "    A very simple evaluator that checks to see if the input of the retrieval step exists\n",
    "    in the retrieved docs.\n",
    "    \"\"\"\n",
    "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"make_handout\")\n",
    "    retrieve_run = next(run for run in rag_pipeline_run.child_runs if run.name == \"retrieval_steps\")\n",
    "\n",
    "    context_dict = retrieve_run.outputs[\"contexts\"]\n",
    "\n",
    "    scores = []\n",
    "    for query, q_c in context_dict.values(): #each document should be an example in the dataset        \n",
    "        for doc in q_c:\n",
    "            time.sleep(60)\n",
    "            scores.append(grade_doc(query, doc))\n",
    "            \n",
    "    return {\"results\": scores}\n",
    "    \n",
    "\n",
    "def faithfulness(root_run, example):\n",
    "    \"\"\"\n",
    "    A simple evaluator that checks to see the answer is grounded in the documents\n",
    "    \"\"\"\n",
    "    # Get documents and answer\n",
    "    rag_pipeline_run = next(run for run in root_run.child_runs if run.name == \"make_handout\")\n",
    "    time.sleep(60)\n",
    "    return grade_faithfulness(rag_pipeline_run, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "617e5b10-8f60-4c24-90f7-222fc7ed6bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_llm_allcases_1(rag_bot):\n",
    "    experiment_results = evaluate(\n",
    "        lambda inputs: rag_bot.make_handout(assessment=inputs[\"assessment\"], md_plan=inputs[\"plan\"]),\n",
    "        data=\"Dummy_pt_case\",\n",
    "        evaluators=[document_relevance, faithfulness],\n",
    "        experiment_prefix=datetime.now().strftime('%Y-%m-%d, %H:%M:%S')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee06f395-6eaf-4cad-87fd-cd5117941b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#eval_llm_allcases_1(rag_bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66492c7b-42d7-4e3c-8cab-c23f0986b01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_llm_allcases_2(rag_bot):\n",
    "    experiment_results = evaluate(\n",
    "        lambda inputs: rag_bot.make_handout(assessment=inputs[\"assessment\"], md_plan=inputs[\"plan\"]),\n",
    "        data=\"Pt_cases\",\n",
    "        #evaluators=[document_relevance, faithfulness],\n",
    "        experiment_prefix=datetime.now().strftime('%Y-%m-%d, %H:%M:%S')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24e4f31d-e601-46f7-a6b1-f228e2729398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: '2024-06-26, 22:19:05-ec8d1c17' at:\n",
      "https://smith.langchain.com/o/edfbc8bb-c3a3-5c1e-8b48-11b5a8cfd8ac/datasets/e3957f7c-e232-4541-beef-d7216ab12241/compare?selectedSessions=d0d6701c-973b-48ee-af7a-d9cef739d05f\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b88dc0612049ae93b0b9921f294ca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_llm_allcases_2(rag_bot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fc3492-d5e1-426e-9428-01d2083cd48e",
   "metadata": {},
   "source": [
    "## LLM grading based on custom metrics\n",
    "- jargon\n",
    "- reference list\n",
    "- template format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a519e-40f6-4cfd-b9f9-27cf48252fa7",
   "metadata": {},
   "source": [
    "## Human feedback of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593e441-f1d7-4521-96a2-ca060923a33d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
